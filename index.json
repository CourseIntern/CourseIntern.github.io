[{"content":"NOTE: Replace \u0026lt;IAM-ROLE-NAME-TASK-1\u0026gt;, \u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt; \u0026amp; \u0026lt;CLUSTER-NAME-TASK-4\u0026gt; with your own values as per your own account.\nTask 1: Create a custom security role\n1gcloud config set compute/zone us-east1-b 2nano role-definition.yaml  Copy the below content to file  1title:\u0026#34;\u0026lt;IAM-ROLE-NAME-TASK-1\u0026gt;\u0026#34;2description:\u0026#34;Permissions\u0026#34;3stage:\u0026#34;ALPHA\u0026#34;4includedPermissions:5- storage.buckets.get6- storage.objects.get7- storage.objects.list8- storage.objects.update9- storage.objects.create Save by : ctrl + o -\u0026gt; enter -\u0026gt; ctrl + x  1gcloud iam service-accounts create orca-private-cluster-sa --display-name \u0026#34;Orca Private Cluster Service Account\u0026#34; 2gcloud iam roles create \u0026lt;IAM-ROLE-NAME-TASK-1\u0026gt; --project $DEVSHELL_PROJECT_ID --file role-definition.yaml Task 2: Create a service account\n1gcloud iam service-accounts create \u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt; --display-name \u0026#34;Orca Private Cluster Service Account\u0026#34; Task 3: Bind a custom security role to an account\n1gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member serviceAccount:\u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/monitoring.viewer 2 3gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member serviceAccount:\u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/monitoring.metricWriter 4 5gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member serviceAccount:\u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/logging.logWriter 6 7gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member serviceAccount:\u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role projects/$DEVSHELL_PROJECT_ID/roles/\u0026lt;IAM-ROLE-NAME-TASK-1\u0026gt; Task 4: Create and configure a new Kubernetes Engine private cluster\n1gcloud container clusters create \u0026lt;CLUSTER-NAME-TASK-4\u0026gt; --num-nodes 1 --master-ipv4-cidr=172.16.0.64/28 --network orca-build-vpc --subnetwork orca-build-subnet --enable-master-authorized-networks --master-authorized-networks 192.168.10.2/32 --enable-ip-alias --enable-private-nodes --enable-private-endpoint --service-account \u0026lt;IAM-SERVICE-ACC-TASK-2\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --zone us-east1-b Step 5: Deploy an application to a private Kubernetes Engine cluster.\n  Navigate to the Compute Engine in the Cloud Console.\n  Click on the SSH button for the orca-jumphost instance.\n  In the SSH window, connect to the private cluster by running the following: Run the following commands in ssh of orca-jumphost:\n1gcloud config set compute/zone us-east1-b 2 3gcloud container clusters get-credentials \u0026lt;CLUSTER-NAME-TASK-4\u0026gt; --internal-ip 4 5kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0 6 7kubectl expose deployment hello-server --name orca-hello-service --type LoadBalancer --port 80 --target-port 8080 Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp342-ensure-access-identity-in-google-cloud/","title":"GSP-342: Ensure Access \u0026 Identity in Google Cloud"},{"content":"Checkout resources for Qwiklabs Diwali Challenge\n ","href":"https://www.courseintern.com/qwiklabs-diwali-2021/","title":"Qwiklabs Diwali 2021"},{"content":"Checkout resources for Learn to Earn Cloud Challenge\n ","href":"https://www.courseintern.com/learn-to-earn-cloud-challenge/","title":"Learn to Earn Cloud Challenge"},{"content":"Hacktoberfest — brought to you by DigitalOcean in partnership with Dev \u0026amp; Intel — is a month-long celebration of open source software. Maintainers are invited to guide would-be contributors towards issues that will help move the project forward, and contributors get the opportunity to give back to both projects they like and others they've just discovered. No contribution is too small — bug fixes and documentation updates are valid ways of participating.\nCan't make it to this online meetup? Hacktoberfest is virtual and open to participants from around the globe. Sign up to participate today\nYou will also get a chance to win lot of goodies and stickers.\nWe also thank our community partners, GitHub Campus Experts.\nPlease fill in this form to register for the event. The seats are filling fast (we already have 200+ registrations till date), so hurry up!!!\nYou'll receive an email with link to join the session which will be held on September 30, 2021 at 5:00 PM IST on YouTube.\n ","href":"https://www.courseintern.com/post/events/hacktoberfest-2021/","title":"Hacktoberfest 2021 "},{"content":"Not only attendees will learn to code a Python program that could create zip-archives from the ground-up, but they will also get a chance to win lot of goodies and stickers.\nWe also thank our community partners, GitHub Campus Experts.\nPlease fill in this form to register for the event. The seats are filling fast (we already have 150+ registrations till date), so hurry up!!!\nYou'll receive an email with link to join the session which will be held on September 5, 2021 at 12:00 PM IST on YouTube.\n ","href":"https://www.courseintern.com/post/events/create-zip-archive-using-python/","title":"Hands on Session On Developing A Zip Archive using Python"},{"content":"HOW TO BECOME AN ASSOCIATE ANDROID DEVELOPER! Hey Folks!\nAssociate Android Developer make your way towards mobile app development. You will be able to learn how to build simple Android apps in Kotlin training. In this, there is no requirement for experience and provide 100% online learning under 10 hours of study a week.\nBenefits of the program  Sharable certificate 100% online courses  Android Basics in Kotlin courses Unit 1: Kotlin Basics Introduction to Kotlin Take your first step in the field of programming in Kotlin.In this, you need to add images and text to your android apps. You will be able to build your first Android app in Kotlin. Then you will be provided with a code lab where you will be provided with the short program in Kotlin using functions and loops to print a happy birthday message. After that, you will be provided a quiz which you need to answer.\n  Create your first app: In this, you will create an app using Android Studio. Firstly you will be given an introduction to Android Studio. Then you need to download and install Android Studio. Then you will be able to create your first Android app. Then you will be able to run your app on your mobile device. After that, you will be provided a quiz which you need to answer.\n  Build a basic layout: In this, you will be able to learn how to add images and text. You will be able to design a Birthday Card app. Then you will be able to add images to your Android app.\n  Unit 2: Layouts In this, you will be able to learn about layouts.\nGet user input in an app Part 1 In this you will be able to learn about two apps to advance your knowledge about UI layouts in Android. So firstly you will be given a bit description about tip calculator. After that, you will be able to learn new UI components, such as editable text fields, radio buttons, and switches to build up the layout for your tip calculator app. You will be able to learn how to write Kotlin code to interact with the UI elements in the tip calculator app so that you can calculate. After that, you will be provided a quiz which you need to answer.\nGet user input in an app Part 2 In this, you can choose the color of your apps and apply them consistently throughout your app using themes. You will be able to change the launcher icon for your app. Then you can update your tip calculator app to look more professional and polished. After that, you will be provided a quiz which you need to answer.\nDisplay a scrollable list Firstly you will be introduced to the Affirmations app and will be creating in this pathway. You will be able to learn how to create lists in Kotlin and loop through them. You will be introduced to RecycleView to display a scrollable list. You will be able to learn how to add images to the scrollable list. After that, you will be provided a quiz which you need to answer.\nUnit 3:Navigation Navigate between screens You will be able to learn basic concepts of app navigation. You will be able to understand collections and how to manipulate them. You will be able to build a Words app with multiple activities. You will be able to learn about the activity lifecycle and different stages of an activity. At last, you need to test your knowledge to earn Navigate between screen badges.\nIntroduction to the Navigation component You will be able to get an introduction to the Navigation component. You will be able to understand how to use fragments and implement navigation. You will be able to introduce MAD skills with the Navigation components. At last, you need to test your knowledge to earn your Introduction to the Navigation component badge.\n  Architecture components: Given a brief introduction about Unscramble app. You need to improve your app by implementing a ViewModel to retain app data. After that, you need to do the quiz for your knowledge.\n  Advanced navigation app examples: You will be able to get an introduction to the Cupcake app which you will create in this pathway. You need to build a cupcake ordering app and use a shared ViewModel. Also, you need to manipulate the back stack in a custom way by modifying the cupcake apps so that the user can cancel the order. At last, you need to test your knowledge to earn an Advanced Navigation app badge.\n  Unit 4: Connect to the Internet   Coroutines: You will firstly be getting introduced to a Coroutines where you will learn to write clear code while building more complex and advanced apps. After that, you will be given a test for your knowledge and then you can earn your Coroutines badge.\n  Get data from the internet: In this, you will be given an introduction to HTTP. You will be able to connect your app to the backend servers. You will learn about REST web services and will be able to load and display images using web URLs. After that, you will be given a test for your knowledge and earn your Get and display data from the internet badge.\n  Unit 5 :Data Persistence   Introduction to SQL, Room, and Flow: In this, you will learn the basics of SQL, how to use and apply relational databases in an Android app. In this, you will learn the fundamentals of relational databases and will be able to practice running SQL queries. In this, you will be able to learn how to work with databases on Android. After that, you will be provided a quiz which you need to answer.\n  Use Room for data persistence: In this, you will learn how to use and test Room Kotlin APIs. In this, you will be able to learn and build an app that uses the room to save inventory items into SQLS queries. You will be able to read, display, update and delete data from the app’s SQLite database using Room. After that, you will be provided a quiz which you need to answer.\n  ","href":"https://www.courseintern.com/post/google-courses/google-associate-android-developer-certification/","title":"Google Associate Android Developer Certification"},{"content":"How to Boost your Google Cloud Skills! Hey Folks!\nIntro Google Cloud Career Readiness program is the 16-week on-demand program that has provided a path for students and faculty in higher education and helps you to prepare for a cloud-first workplace by making you ready for industry-recognized certifications. This program will help you to learn cloud computing fundamentals with hands-on practice in labs. With on-demand training on Coursera and Qwiklabs, you will build skills in topics related to cloud infrastructure, data analytics, and machine learning. The main objective of this program is to help the students to learn the essentials of the Google Cloud Platform and to explore this program. This program is very useful for the students who had a great dedication and willingness to explore their career in Google Cloud Program. This program is open for the students of all the Engineering streams of 3rd and 4th year, provided they should have high interest in making their career in Cloud Infrastructure. This program is specially designed to provide students with training, mentorship, and certification to launch their careers with companies seeking cloud talent. One should not pass over the chance to advance their career by learning Google Cloud at your own pace, earning badges and certifications, and building projects. Google Cloud Career Readiness program is the journey to different certifications of Google Cloud. Google Cloud Career Readiness program offers provides the students to enhance their knowledge and career in this field. Career readiness programs develop logical reasoning skills to synthesize existing connections among various factors to solve problems. Apply for the career readiness program and help transform the future of the students. Google Cloud Career Readiness program leads the students to prepare them for future careers.\nBenefits Google Cloud Career Readiness program has the following benefits :\n This program provides students 100% Coursera. This will provide 50% off on the ACE exam.  Significance This program also has much significance as it identifies the caliber of the students as sometimes students at a younger age are often not aware of their true potential. This program creates opportunities that help them to analyze the current scenario of the job market. This program was quite unheard of until recent days. Critically analyzing the current market scope and skills possessed by the younger graduates, these programs are now given the utmost priority in every educational sector. A vision of future, career, Career Readiness Programs makes students industry-ready even before the completion of their graduation. Students nowadays never get a glimpse of the real-world waiting outside for them, but with thorough practice and determination, the students can polish their skills to make their future in Clouds.\nWho can facilitate in the program In this program, only teachers are eligible to Facilitate. If you are an interested student, please ask your professor to apply for it.\nTracks This program offers an engaging platform for students to enhance their skills through two tracks: the Associate Cloud Engineer Track and the Data Analyst Track. If completed any one of the tracks then you can go for other tracks. The following are tracks of the Google Cloud Readiness Program.\n The Associate Cloud Engineer track. Data Analyst Track  The Associate Cloud Engineer track is the 16-week track that provides students the opportunity to pursue their career in engineering and management roles related to cloud infrastructure, cloud-native application development, and data engineering. This track gives free access to architecting with Google Cloud platform specialization on Coursera which provides up to 40 hours of content...This on-demand training includes six-in depth courses with videos, hands-on labs, and quizzes. After completing the Specialization, students can earn the Architecting with Google compute engine Specialization certificate. Based on other completion timelines and other criteria, students receive a 50% discount on the certification exam whereas faculty receives a 100% discount. Associate Cloud Engineer develops applications, monitors operating, and manages enterprises solution. Associate Cloud Engineer exam assesses the ability to plan and configure a cloud solution. It ensures the successful operation of a cloud solution. It set up a cloud solution environment. It configures access and security. As you know that hands-on experience is crucial for success with the Associate Cloud Engineer Certification exam. In this, you will get credits to complete the labs on the Qwiklabs platform free of charge. This training will put you on the path to success for achieving the Associate Cloud Engineer track certification. This all will test your cloud skills.\nAbout Certification Exams The Certifications exam will be of 2 hours paper having registration fees of $125 (plus tax where applicable). The languages for this paper will be English, Japanese, Spanish. The format of these exams will be multiple choice questions and multi-select questions. The recommended experience should be 6 months + hands-on experience with Google Cloud.\nAfter completing the certification exam, you will get access to digital badges and credentials which will help you to build your resume and unlock new job opportunities. Institutions around the world partnered with Google Cloud to solve student success.\nSuccess with the Associate Cloud Engineer track  Operating systems and file systems. Working with Linux at the command line and editing text files from the Linux command line. -To participate in this track you will need a laptop or desktop with the latest versions of any popular web browser. The Google Chrome browser is recommended. An internet connection to access the Google Cloud Platform console is required.  Data Analyst track is a 16-week track available for students who want to pursue their careers in data analytics, business intelligence, and management.\nIn this track, students will get video-based training on Coursera and which will provide 30 hours of content in Google Cloud Platform Specialization. Students will get one benefit in doing this track as they will get free access to hands-on practice on labs. On completing the Specialization, students will be able to earn the from DATA to insights with this Google Cloud Platform Specialization certificate. To complete this Data Analyst track one should know well the ANSI SQLS and statistics.\n","href":"https://www.courseintern.com/post/google-courses/google-cloud-career-readiness-program/","title":"Google Cloud Career Readiness Program"},{"content":"Enriching Life in Data Analytics! Data Analytics is the process of analyzing raw data so that we can pull out insights that are useful to companies. If you want to mound your career and gain the job-ready skills you need to advance your career in Data Analytics. In this, there is no requirement for experience and provide 100% online learning under 10 hours of study a week. Nowadays there are in-demand jobs in the field of Data Analytics with a professional certificate from Google. You will be able to gain and understand the processes used by Data Analytics their day-to-day job. You will be able to understand how to get clear and organize data for analysis.\nBenefits of the program  Sharable certificate 100% online courses Beginner level Flexible Schedule  Enrollment: You can enroll in this program as its enrollment starts on 26 July 2021.\nProfessional Certificate To boost your skills to start a new career, Professional Certificate on Coursera helps you to become job-ready. Enroll in this program and you will get a 7-day free trial. You can pause and end your subscription at any time. You have to apply your skills with hands-on projects that manifest your job readiness to potential employers. To earn the certificate, you need to finish the project that you can share with your professional network. This may help you to kickstart your career in this program. Professional Certificate may also help you to prepare for the certification exam.\nThere are 8 Courses in the Professional Certificate  Foundations: Data, Data, Everywhere Ask Questions to Make Data-Driven Decisions Prepare Data for Exploration Process Data from Dirty to Clean Analyze Data to Answer Questions Share Data Through the Art of Visualization Data Analysis with R Programming  Foundations: Data, Data, Everywhere This is the course of the Google Data Analytics Certificate. In this course, you will be able to enhance your skills and will be able to bring into the world of data analytics with a hands-on curriculum developed by Google. In this course, you will be getting brief knowledge about key data analytics topics and data Analytics which improves their processes and identifies opportunities and trends. You will be getting knowledge about key analytical skills which include data cleaning, data analysis, data visualization.\nYou will be able to  Understand the process of how Data Analytics do in their day-to-day job. Understand the role of analytics in the Data ecosystem. Explore job opportunities. Understand key analytical skills.  Ask Questions to Make Data-Driven Decisions This is the course in the Google Data Analytics Certificate. In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. You will be built-in with the topics that were introduced to you in the first course of the Google Data Analytics Certificate. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nYou will be able to   Learn about basic techniques that can help in Data Analysis.\n  Explore real-world business scenarios.\n  Help analysts to learn and understand problems.\n  Give a better understanding of data-driven decision-making.\n  Prepare Data for Exploration In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. You will be built-in with the topics that were introduced to you in the first and second course of the Google Data Analytics Certificate. In this course, you will be introduced to new topics which will enhance practical data analytics skills. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nYou will be able to  Learn best practices for organizing and securing data. Understand how to access database and extract filter. Understand some terms like structured and unstructured data. Understand the importance of data ethics and data privacy.  Process Data from Dirty to Clean In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. In this course, you will continue to build and understand data analytics. You will be also clear with the concepts and tools that data analysts use in their work. You will be exploring the “analyze” phase of the data analysis process. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nYou will be able to  Check for data integrity. Develop basic SQL queries Explore the elements of data cleaning reports. Understand the results of cleaning data.  Analyze Data to Answer Questions In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. You will learn how to apply the analysis to make sense of the data you have collected. You will be able to learn how to use formulas, SQL queries, and functions. You will be enhancing your knowledge of how to solve complex calculations on your data. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nYou will be able to  Learn how to organize data. Learn how to complete calculations using SQL. Learn the process for adjusting data. Understand how to aggregate data in spreadsheets.  Share Data Through the Art of Visualization This is the course in the Google Data Analytics Certificate. In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. In this course, you will learn how to visualize and present your data. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nScholars who complete this certificate program will be provided to apply as data analysts.\nYou will be able to:  Learn the importance of data visualization. Understand how to use Tableau to create the visualization. Understand how to use Tableau to create dashboards. Explore the principles involved with effective presentations.  Data Analysis with R Programming This is the course in the Google Data Analytics Certificate. In this course, you will enhance the skills needed to apply to introductory-level data analyst jobs. In this course, You’ll find out how to use RStudio.RStudio makes the environment that allows you to work with R.This also covers software applications and tools that are unique to R, such as the R package. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nScholars who complete this certificate program will be provided to apply as data analysts.\nYou will be able to  Learn the benefits of the R programming language. Understand how to use RStudio to apply R to your analysis. Learn about R Markdown. Understand the data frames and their uses in R.  Google Data Analytics Capstone: Complete a Case Study This is the last and final course in the Google Data Analytics Certificate. In this course, you will enhance your skills and will provide you with opportunities to complete an optional case study, which will help prepare you for the data analytics job hunt. You will be instruct and yield with hands-on ways to bring out common data analyst tasks.\nScholars who complete this certificate program will be provided to apply as data analysts.\nYou will be able to  Learn benefits and uses of case studies. Explore real-world business scenarios. Examine different case study scenarios. Understand how case studies can be a part of the job interview process.  ","href":"https://www.courseintern.com/post/google-courses/google-data-analytics-professional-certificate/","title":"Google Data Analytics Professional Certificate"},{"content":"Skillshare in UX design! Intro UX design is the behavior and the patterns and habits of people to make their experience better. This is to your path to a career in UX Design. In this program, you will be able to learn in-demand skills. This program provides you with job-ready skills to start or advance your career. This will enhance your knowledge and gain an immersive understanding of practices in this field. This program doesn’t require any degree and experience. After enrolling in this program one can understand the basics of UX research like planning research studies, conducting interviews and usability studies, and synthesizing research results. This prepares you for a career in the high-growth field of UX design. UX designers focus on users’ interaction with products, like websites, apps, and physical objects.\nWhat you will learn You will be able to follow the design process i.e empathize with users, define pain points, ideate solutions, create wireframes and prototypes, test and iterate on designs You can easily apply foundational UX concepts, like user-centered design, accessibility, and equity-focused design.\nSkills you will gain  User Experience (UX) UX Research User Experience Design (UXD) UX design jobs  Enrollment You can enroll in this program as its enrollment starts on 9 July 2021\nProfessional Certificate To boost your skills to start a new career, Professional Certificate on Coursera helps you to become job-ready. Enroll in this program and you will get a 7-day free trial. You can pause and end your subscription at any time. You have to apply your skills with hands-on projects that manifest your job readiness to potential employers. To earn the certificate, you need to finish the project that you can share with your professional network. This may help you to kickstart your career in this program. Professional Certificate may also help you to prepare for the certification exam.\nThere are 7 Courses in the Professional Certificate  Foundations of User Experience (UX) Design UX design process: Empathize, Define, Ideate Build Wireframes and Low-Fidelity Prototypes Conduct UX Research and Test Early Concepts Create High-Fidelity Designs and Prototypes Responsive Web Design in Adobe XD Design a User Experience  Foundations of User Experience (UX) Design Foundations of User Experience (UX) Design is the first of all the seven courses that will equip you with the skills and boost your career in this field. UX design focus on the interactions that people have with the product like websites, mobile apps, and physical objects. This will help you to complete the hands-on activities related to the UX designer scenario. In this program, you can identify the factors that contribute to great user experience design. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Define the field of UX and why it’s important. Explore job opportunities and career paths Participate in online UX communities. Why design sprints are important and useful?  UX design process: Empathize, Define, Ideate: UX design process Empathize, Define, Ideate is second in all seven courses that will enhance you with the skills needed to apply to entry-level jobs, and in this course, you will complete the first phase of the design process and this will help you to complete the hands-on activities related to the UX designer scenario. In this course; you can empathize with users to understand their needs and pain points. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Common UX research methods. Conduct competitive audits. Start designing a mobile app Develop problem statements to define user needs  Build Wireframes and Low-Fidelity Prototypes This Is the third course that will provide you with the skills you need to apply to entry-level jobs in user experience (UX) design. In this, you will continue to design a mobile app. In this, you will be able to create storyboards and will be familiar with drawing too. You will be able to create paper wireframes and digital wireframes using the design tool Figma. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to:  Develop a goal statement. Apply the basics of drawing. Design a low-fidelity prototype in Figma. Create two types of storyboards  Conduct UX Research and Test Early Concepts This Is the fourth course that will provide you with the skills you need to apply to entry-level jobs in user experience (UX) design. In this course, you will be able to study and gather feedback about designs. Along with this, you will be able to modify your low-fidelity designs. This will help you to complete the hands-on activities related to the UX designer scenario. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Plan a UX research study, Create affinity diagrams for withdrawing the group and analyze data. Modify low-fidelity designs Take notes during a usability study.  For certification, you need to complete the previous three courses or you need to have the ability to conduct user research to inform the creation of empathy maps, personas, user stories, user journey maps, and problem statements.\nCreate High-Fidelity Designs and Prototypes This is the fifth course that will provide you with the skills you need to apply to entry-level jobs in user experience (UX) design. In this course, you need to follow the step-by-step tutorials to learn how to create high-fidelity designs. Afterward, you’ll be able to learn how to share your designs with development teams and highlight your work in your professional UX portfolio. t will help you to complete the hands-on activities related to the UX designer scenario.\nScholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Apply common visual design elements Exhibit how to design system can be organized. Build mockups and high-fidelity designs Role of design critique sessions  A learner who has not completed the previous courses should have a complete understanding of the design process. This course is suitable for beginners who have completed the previous four courses of the Google UX Design Certificate.\nResponsive Web Design in Adobe XD Responsive Web Design in Adobe XD is the sixth course that will equip you with the skills you need to apply to entry-level jobs in user experience (UX) design. It will help you to complete the hands-on activities related to the UX designer scenario. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Work with design systems in Adobe XD. Apply common layouts for web pages. Concern about the UX design process Create or update a UX-focused resume  A learner who has not completed the previous courses should have a complete understanding of the design process. This course is suitable for beginners who have completed the previous four courses of the Google UX Design Certificate.\nDesign a User Experience Design a User Experience is the last course that will enhance you with the skills you need to apply to entry-level jobs in user experience (UX) design. In this course, you will design a dedicated mobile app and a responsive website too. It will help you to complete the hands-on activities related to the UX designer scenario. Scholars who complete the program should be appointed to apply for entry-level jobs as UX designers.\nYou will be able to  Define each step of the UX design process Interview for an entry-level UX design job. Prepare you for job applications. Build wireframes and mockups.  ","href":"https://www.courseintern.com/post/google-courses/google-ux-design-professional-certificate/","title":"Google UX Design Professional Certificate"},{"content":"Training to be the best! Hey Folks, read on more to find out about Google project management professional certificate.\nIntro Project Management is the application of knowledge, skills, tools, and techniques to meet the project requirements and achieve the desired outcome. In this course, you will learn how to hone the skills to become a truly great project manager and the awesome thing about project management is that it spans a lot of industries and company types as it doesn’t require deep technical knowledge. Google Project Management certificate provides you with job-ready skills to start or advance your career in project management and get access to practice exercises, quizzes, discussion forums job search help, and more on Coursera. This will enhance your knowledge and gain an immersive understanding of practices in this field. You can also build your new career in the high-growth field of project management. In this, you don’t need any experience as well as a degree.\nWhat you will learn  Learn how to make effective project documentation. Practice problem-solving, strategic communication, and stakeholder management through real-world scenarios.  Enrollment You can enroll in this program as its enrollment starts on 2 July 2021\nProfessional Certificate To boost your skills to start a new career, Professional Certificate on Coursera helps you to become job-ready. Enroll in this program and you will get a 7-day free trial. You can pause and end your subscription at any time. You have to apply your skills with hands-on projects that manifest your job readiness to potential employers. To earn the certificate you need to finish the project which you can share with your professional network as well. This may help you to kickstart your career in this program. Professional Certificate may also help you to prepare for the certification exam. There are 6 Courses in the Professional Certificate:\nFoundation of project management This course is the first in the series of six courses that may accoutre you all with the skills that you need to apply to the introductory-level roles in project management. The Project manager plays an important role in leading planning and facilitates the critical project to help the organization succeed. In this course, you will be able to gain a deeper understanding of the role and responsibilities of a project manager. You may also familiarize the kinds of jobs that you can pursue after the completion of the program. Scholars who complete the program should be appointed to apply for introductory-level jobs as project managers.\nYou will be able to:  Define and explore project management roles and responsibilities. Grab skills on how to become a successful project manager. Define change management and the role of the project manager in this process. Acknowledge organizational structure and culture and its impact.  Project Initiation: Starting a Successful Project: This is the second course in the series of six courses of the Google Project Management Certificate program. This course will show how to set up a project for success in the first phase of the project. In this course, you will grab how to define and manage project goals, deliverables, scope, and success criteria. Project managers will guide you and provide you with hands-on approaches for accomplishing these tasks. They will show you the best project management tools and resources for the job at hand. Scholars who complete the program should be appointed to apply for introductory-level jobs as project managers.\nYou will be able to  Determine the project's benefits and costs. Complete a stakeholder analysis Define project goals and deliverables. Understand the key components of the project initiate phase.  Project Planning Putting it all together: This is the third course in the series of Google Project Management Certificate programs. In this course; you will explore how to map out a project in the second phase of the project. You will be able to grab how to build and manage a budget. In this, you will grab how to draft and manage communication plans and you will also be able to acknowledge how to organize project documentation. The project manager will instruct and provide you with hands-on approaches for completing these tasks. Scholars who complete the program should be appointed to apply for introductory-level jobs as project managers. In this course, no previous experience is required.\nYou will be able to  Explain why milestones are important. Explain why the project plan is necessary. Describe how to manage and maintain a budget. Identifies tools and best practices to build a project plan.  Project Execution: Running the Project This is the fourth course out of the six series of the Google Project Management Certificate program. In this course; you will explore the closing phase of the project life cycle. You will learn how to manage, communicate changes and risks. You will also examine how to compute data, how to use data to inform your decision-making, and how to effectively present that data. The project manager will instruct and provide you with hands-on approaches for completing these tasks. Scholars who complete the program should be appointed to apply for introductory-level jobs as project managers.\nYou will be able to  Explain the purpose of retrospective. Describe the closing process of stakeholders. Demonstrate how to analyze the data. Compare the different tracking methods.  Agile Project Management This is the fifth course in the series of the six Google Project Management Certificate programs. This course will explore the history and philosophy of Agile project management. In this, you will learn about Syrum. You will learn about exploring its pillars also. You will also be able to grab how to search for and land opportunities in Agile roles. The project manager will instruct and provide you with hands-on approaches for completing these tasks.\nScholars who complete the program should be appointed to apply for introductory-level jobs as project managers. In this course, no previous experience is required.\nYou will be able to  Explain the pillars of Syrum. Implement Agile’s value-driven delivery strategies Guide a job search for an Agile role Build and manage a Product Backlog  Capstone: Applying Project Management in the Real World This is the last course in the Google Project Management Certificate program. In this final course, you will practice applying project management knowledge and skills. To start with this course learners should complete Course 1-5 as they provided the foundation necessary to complete the activities in this course. After completing this course one can apply to apply for introductory-level jobs as a project manager. In this course, you have the opportunity to claim a certification of completion badge that will be recognizable to employers.\nYou will be able to  Develop a portfolio of project management artifacts Manage stakeholders and teams. Organize plans, and communicate project details. Prepare for the interview for project management jobs  ","href":"https://www.courseintern.com/post/google-courses/google-project-management-professional-certificate/","title":"Google Project Management Professional Certificate"},{"content":"Get more out of now! IT support is for the students who want to build their career in IT and they will be able to gain the skills required to succeed in an entry-level IT capacity. Nowadays there are in-demand jobs in the field of IT with a professional certificate from Google. In this, there is no requirement for experience and provide 100% online learning under 10 hours of study a week. If you want to mound your career and gain the job-ready skills you need to advance your career in the IT field\nFIND THE PROGRAMS AVAILABLE  Launch a new career in IT Advance your IT career  Launch a new career in IT Google IT Support Professional Certificate\nIn this, you need to Prepare for an entry-level role in IT support by earning the Google IT Support Professional Certificate. No experience is required to get started. In an IT career, you need to design to take you from beginner to job-ready in under six months. You need to complete hands-on practice to help them to make their career in this field.\nHow to enroll Google IT Support Professional Certificate\nThis will path your career in the IT field and you will become job-ready in less than 6 months. In this, you will learn how to provide end-to-end customer support, ranging from identifying problems to troubleshooting and debugging. You will be able to learn to perform day-to-day IT support tasks.\nBenefits of the program  Sharable certificate 100% online courses Beginner level  Enrollment You can enroll in this program as its enrollment starts on 17 July 2021.\nProfessional Certificate To boost your skills to start a new career, Professional Certificate on Coursera helps you to become job-ready. Enroll in this program and you will get a 7-day free trial. You can pause and end your subscription at any time. You have to apply your skills with hands-on projects that manifest your job readiness to potential employers. To earn the certificate, you need to finish the project that you can share with your professional network. This may help you to kickstart your career in this program. Professional Certificate may also help you to prepare for the certification exam.\nThere are 5 Courses in the Professional Certificate  Technical Support Fundamentals The Bits and Bytes of Computer Networking Operating Systems and You: Becoming a Power User System Administration and IT Infrastructure Services IT Security: Defense against the digital dark arts  Technical Support Fundamentals Technical Support Fundamentals is the first course that aims to prepare you for a role as an entry-level IT Support Specialist. In this course, you will be able to learn different facts about IT like computer hardware, the Internet, and computer software.\nYou will be able to  Understand how the binary system works. Understand what the Internet is. Understand how applications are created and how it works. Utilize soft skills in an Information Technology setting.  The Bits and Bytes of Computer Networking The Bits and Bytes of Computer Networking is the second course to provide a full overview of computer networking. In this course, you will be able to learn how to describe computer networks in terms of a five-layer model.\nYou will be able to  understand cloud computing and cloud storage. learn network services like DNS and DHCP understand all of the standard protocols Learn the fundamentals of modern networking technologies.  Operating Systems and You: Becoming a Power User It is the third course that will provide hands-on practice. In this course, you will learn the main components of an operating system.\nYou will be able to  Performing tasks like managing software and users. Install and remove software on the Windows and Linux operating systems. Understand how system processes work. Understand how to manage the system.  System Administration and IT Infrastructure Services System Administration and IT Infrastructure Services is the fourth course that will work on a single computer to an entire fleet. In this course, you will learn how to manage and configure servers. You will also learn about how to recover the organization's IT infrastructure.\nYou will be able to  Manage the tools that your organization will use Understand how to manage infrastructure servers Practices for selecting services for the organization. Manage an organization’s computers  IT Security: Defense against the digital dark arts It is the last course that covers a wide variety of IT security concepts, tools, and best practices. In this course, you will cover network security solutions, ranging from firewalls to Wifi encryption options. There are three As of information security: authentication, authorization, and accounting.\nYou will be able to  Difference between authentication and authorization. Get Best practices for securing a network. Learn Various authentication systems and types. Grab how to grasp security concepts.  Advance your IT career In this, you will be able to learn Python Programming and explore how to apply it in the IT field. You will be able to practice your technical skills with hands-on projects and assessments. You need to prepare for a variety of opportunities in IT. In this you will be able to learn tasks by writing a Python script. You will be able to manage IT resources at scale, both for physical machines and virtual machines in the cloud. You will be able to learn how to use Git and GitHub.\nThere are 6 Courses in the Professional Certificate\n Crash Course on Python Using Python to interact with the Operating System Introduction to Git and Github Troubleshooting and Debugging Techniques Configuration Management and the Cloud Automating Real-World Tasks with Python  Crash Course on Python It is the course that will teach you how to write simple programs in python using the most common structure. You will be able to learn the benefits of programming in IT roles. Along with that, you will be able to get hands-on experience with programming concepts through interactive exercises and real-life examples.\nUsing Python to interact with the Operating System It is the course in which you will be able to manipulate files and processes on your computer’s operating system. You will practice using the Linux command line on a virtual machine.\nIntroduction to Git and Github In this course, you will be able to get about Git, how to keep track of the different versions of your code. You will be able to reserve your code's history in Git and get together with others in GitHub.\nTroubleshooting and Debugging Techniques In this course, you will be solving real-life problems that you might come across in your IT role. You will be getting problems that can affect any operating system. You will be looking into technical problems and see how to apply them to solve different real-world scenarios.\nConfiguration Management and the Cloud In this course, you will be given a brief how-to apply automation to manage fleets of computers. You will also read some new terms like configuration management, which means the automation technique, that lets you manage the configuration of our computers at scale. You will also learn how to use Puppet.\nAutomating Real-World Tasks with Python You will learn all the concepts you have learned until now in this course. You will be acknowledged about Qwiklabs that will challenge you to use multiple skills at once. You will be able to describe a problem and use those skills to create a solution. At last, you will be given a project in which you need to describe what your customer needs, and you need to create a program to do it!\n","href":"https://www.courseintern.com/post/google-courses/google-it-support-professional-certificate/","title":"Google IT Support Professional Certificate"},{"content":"Checkout resources for 30 Days of Google Cloud Program\n ","href":"https://www.courseintern.com/30-days-of-google-cloud-program/","title":"30 Days of Google Cloud Program"},{"content":"Checkout resources for Google Cloud Ready Facilitator Program\nGet your Google Cloud Ready Facilitator badge from here: https://www.courseintern.com/google-cloud-ready-facilitator-badge-creator/\n Make sure to like the videos that you reference from the doc.\nCheers!\n","href":"https://www.courseintern.com/google-cloud-ready-facilitator-program/","title":"Google Cloud Ready Facilitator Program"},{"content":"Please fill in this form and we'll reach out to you.\n ","href":"https://www.courseintern.com/campus-leaders/","title":"Hiring Campus Leaders"},{"content":"Train the first model Train the first model on the complete dataset. Use train_data for your data and train_labels for you labels.\n1model = Sequential() 2model.add(layers.Dense(200, input_shape=(input_size,), activation=\u0026#39;relu\u0026#39;)) 3model.add(layers.Dense(50, activation=\u0026#39;relu\u0026#39;)) 4model.add(layers.Dense(20, activation=\u0026#39;relu\u0026#39;)) 5model.add(layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) 6model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 7model.fit(train_data, train_labels, epochs=10, batch_size=2048, validation_split=0.1) Train your second model Train your second model on the limited dataset. Use limited_train_data for your data and limited_train_labels for your labels. Use the same input_size for the limited_model\n1create the limited_model = Sequential() 2limited_model.add (your layers) 3limited_model.compile 4limited_model.fit 1limited_model = Sequential() 2limited_model.add(layers.Dense(200, input_shape=(input_size,), activation=\u0026#39;relu\u0026#39;)) 3limited_model.add(layers.Dense(50, activation=\u0026#39;relu\u0026#39;)) 4limited_model.add(layers.Dense(20, activation=\u0026#39;relu\u0026#39;)) 5limited_model.add(layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) 6limited_model.compile(loss=\u0026#39;mean_squared_error\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 7limited_model.fit(limited_train_data, limited_train_labels, epochs=10, batch_size=2048, validation_split=0.1) Add WithConfigBuilder code 1config_builder = (WitConfigBuilder( 2 examples_for_wit[:num_datapoints],feature_names=column_names) 3 .set_custom_predict_fn(limited_custom_predict) 4 .set_target_feature(\u0026#39;loan_granted\u0026#39;) 5 .set_label_vocab([\u0026#39;denied\u0026#39;, \u0026#39;accepted\u0026#39;]) 6 .set_compare_custom_predict_fn(custom_predict) 7 .set_model_name(\u0026#39;limited\u0026#39;) 8 .set_compare_model_name(\u0026#39;complete\u0026#39;)) 9WitWidget(config_builder, height=800) Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp324-explore-machine-learning-models-with-explainable-ai/","title":"GSP-324 Explore Machine Learning Models With Explainable AI"},{"content":"Built Docker image with tag V1 1gsutil cp gs://sureskills-ql/challenge-labs/ch04-kubernetes-app-deployment/echo-web.tar.gz . OR\n1gsutil cp gs://$DEVSHELL_PROJECT_ID/echo-web.tar.gz . Extract downloaded zip 1tar -xvf echo-web.tar.gz 2 3gcloud builds submit --tag gcr.io/$DEVSHELL_PROJECT_ID/echo-app:v1 . Create Kubernetes cluster 1gcloud container clusters create echo-cluster --num-nodes 2 --zone us-central1-a --machine-type n1-standard-2 Deploy application to Kubernetes cluster 1kubectl create deployment echo-web --image=gcr.io/qwiklabs-resources/echo-app:v1 Expose app to allow external access 1kubectl expose deployment echo-web --type=LoadBalancer --port=80 --target-port=8000 Extract Kubernetes services 1kubectl get svc Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp304-build-and-deploy-a-docker-image-to-a-kubernetes-cluster/","title":"GSP-304 : Build and Deploy a Docker Image to a Kubernetes Cluster"},{"content":"For our first session, we have invited an International speaker and a veteran open source contributor who was also a Google Summer Of Code (GSoC) mentor, to provide us a hands-on session. This session will span approx 3 hours, for coding a full-fledged weather app from scratch. The speaker has a Master's degree from Georgia Tech, US (one of the top-10 CS graduate programmes in the world) with a specialization in Machine Learning.\nNot only attendees will learn to code a Python/Django app from ground-up, but they can also get a chance to win lot of goodies and stickers.\nWe also thank our community partners, GitHub Campus Experts.\nPlease fill in this form to register for the event. The seats are filling fast (we already have 100+ registrations till date), so hurry up!!!\nYou'll receive an email with link to join the session which will be held on June 13, 2021 at 12:30 PM IST.\n ","href":"https://www.courseintern.com/post/events/hands-on-session-weather-app-python-django/","title":"Hands on Session On Developing A Weather App in Python/Django"},{"content":"Course Intern team includes Technologists, perpetual students, teachers who focuses on continual incremental improvement. We provide information about free courses, internships and swags.\nYou can contact our team on: \n","href":"https://www.courseintern.com/about/","title":"About"},{"content":"Task - 1 : Create development VPC manually 1gcloud compute networks create griffin-dev-vpc --subnet-mode custom 2 3gcloud compute networks subnets create griffin-dev-wp --network=griffin-dev-vpc --region us-east1 --range=192.168.16.0/20 4 5gcloud compute networks subnets create griffin-dev-mgmt --network=griffin-dev-vpc --region us-east1 --range=192.168.32.0/20 Task - 2 : Create production VPC manually 1gsutil cp -r gs://cloud-training/gsp321/dm . 2 3cd dm 4 5sed -i s/SET_REGION/us-east1/g prod-network.yaml 6 7gcloud deployment-manager deployments create prod-network \\ 8 --config=prod-network.yaml 9 10cd .. Task - 3 : Create bastion host 1gcloud compute instances create bastion --network-interface=network=griffin-dev-vpc,subnet=griffin-dev-mgmt --network-interface=network=griffin-prod-vpc,subnet=griffin-prod-mgmt --tags=ssh --zone=us-east1-b 2 3gcloud compute firewall-rules create fw-ssh-dev --source-ranges=0.0.0.0/0 --target-tags ssh --allow=tcp:22 --network=griffin-dev-vpc 4 5gcloud compute firewall-rules create fw-ssh-prod --source-ranges=0.0.0.0/0 --target-tags ssh --allow=tcp:22 --network=griffin-prod-vpc Task - 4 : Create and configure Cloud SQL Instance 1gcloud sql instances create griffin-dev-db --root-password password --region=us-east1 2 3gcloud sql connect griffin-dev-db 4 5CREATE DATABASE wordpress; 6GRANT ALL PRIVILEGES ON wordpress.* TO \u0026#34;wp_user\u0026#34;@\u0026#34;%\u0026#34; IDENTIFIED BY \u0026#34;stormwind_rules\u0026#34;; 7FLUSH PRIVILEGES; 8 9exit Task - 5 : Create Kubernetes cluster 1gcloud container clusters create griffin-dev \\ 2 --network griffin-dev-vpc \\ 3 --subnetwork griffin-dev-wp \\ 4 --machine-type n1-standard-4 \\ 5 --num-nodes 2 \\ 6 --zone us-east1-b 7 8 9gcloud container clusters get-credentials griffin-dev --zone us-east1-b 10 11cd ~/ 12 13gsutil cp -r gs://cloud-training/gsp321/wp-k8s . Task - 6 : Prepare the Kubernetes cluster  Open Editor -\u0026gt; wp-k8s -\u0026gt; wp-env.yaml Change username and password to:  1username :wp_user2password :stormwind_rules Save.  1cd wp-k8s 2 3kubectl create -f wp-env.yaml 4 5gcloud iam service-accounts keys create key.json \\ 6 --iam-account=cloud-sql-proxy@$GOOGLE_CLOUD_PROJECT.iam.gserviceaccount.com 7kubectl create secret generic cloudsql-instance-credentials \\ 8 --from-file key.json Task - 7 : Create a WordPress deployment   In editor: wp-deployment.yaml -\u0026gt; replace YOUR_SQL_INSTANCE with griffin-dev-db.\n  Save.\n  1kubectl create -f wp-deployment.yaml 2kubectl create -f wp-service.yaml Task - 8 : Enable monitoring   Navigation Menu -\u0026gt; Kubernetes Engine -\u0026gt; Services and Ingress -\u0026gt; Copy Endpoint's address.\n  Navigation Menu -\u0026gt; Monitoring -\u0026gt; Uptime Checks -\u0026gt; + CREATE UPTIME CHECK\n  1Title :Wordpress Uptime Next -\u0026gt; Target  1Hostname :{Endpoint\u0026#39;s address} (without http...)2Path :/ Next -\u0026gt; Next -\u0026gt; Create  Task - 9 : Provide access for an additional engineer  Navigation Menu -\u0026gt; IAM \u0026amp; Admin -\u0026gt; IAM -\u0026gt; ADD  1New Member :{Username 2 from Lab instruction page}2Role :Project -\u0026gt; Editor Save.  Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp321-set-up-and-configure-a-cloud-environment-in-google-cloud/","title":"GSP-321: Set Up and Configure a Cloud Environment in Google Cloud"},{"content":"Start by executing the following commands:\n1gcloud config set project $(gcloud projects list --format=\u0026#39;value(PROJECT_ID)\u0026#39; --filter=\u0026#39;qwiklabs-gcp\u0026#39;) 2git clone https://github.com/rosera/pet-theory.git Task - 1: Firestore Database Create Go to Firestore \u0026gt; Select Naive Mode \u0026gt; Location: nam5 \u0026gt; Create Database\nTask - 2: Firestore Database Populate 1cd pet-theory/lab06/firebase-import-csv/solution 2npm install 3node index.js netflix_titles_original.csv Task - 3: Cloud Build Rest API Staging 1cd ~/pet-theory/lab06/firebase-rest-api/solution-01 2npm install 3gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/rest-api:0.1 4gcloud beta run deploy netflix-dataset-service --image gcr.io/$GOOGLE_CLOUD_PROJECT/rest-api:0.1 --allow-unauthenticated  Choose 1 and us-central1  Task - 4: Cloud Build Rest API Production 1cd ~/pet-theory/lab06/firebase-rest-api/solution-02 2npm install 3gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/rest-api:0.2 4gcloud beta run deploy netflix-dataset-service --image gcr.io/$GOOGLE_CLOUD_PROJECT/rest-api:0.2 --allow-unauthenticated  Goto cloud run and click netflix-dataset-service then copy the url.  1SERVICE_URL=\u0026lt;copy url from your netflix-dataset-service\u0026gt; 2curl -X GET $SERVICE_URL/2019 Task - 5: Cloud Build Frontend Staging 1cd ~/pet-theory/lab06/firebase-frontend/public 2nano app.js # comment line 3 and uncomment line 4, insert your netflix-dataset-service url 3npm install 4cd ~/pet-theory/lab06/firebase-frontend 5gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/frontend-staging:0.1 6gcloud beta run deploy frontend-staging-service --image gcr.io/$GOOGLE_CLOUD_PROJECT/frontend-staging:0.1  Choose 1 and us-central1  Task - 6: Cloud Build Frontend Production 1gcloud builds submit --tag gcr.io/$GOOGLE_CLOUD_PROJECT/frontend-production:0.1 2gcloud beta run deploy frontend-production-service --image gcr.io/$GOOGLE_CLOUD_PROJECT/frontend-production:0.1  Choose 1 and us-central1  Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp344-serverless-firebase-development/","title":"GSP-344: Serverless Firebase Development"},{"content":"Start by executing the following commands:\n1gcloud auth list 2gcloud config list project 3 4gcloud config set project \\ 5$(gcloud projects list --format=\u0026#39;value(PROJECT_ID)\u0026#39; \\ 6--filter=\u0026#39;qwiklabs-gcp\u0026#39;) 7 8gcloud config set run/region us-central1 9gcloud config set run/platform managed 10 11git clone https://github.com/rosera/pet-theory.git \u0026amp;\u0026amp; cd pet-theory/lab07 12export PROJECT_ID=$(gcloud info --format=\u0026#39;value(config.project)\u0026#39;) Task - 1 : Deploy a Public Billing Service 1cd ~/pet-theory/lab07/unit-api-billing 2 3gcloud builds submit --tag gcr.io/${PROJECT_ID}/billing-staging-api:0.1 4gcloud run deploy public-billing-service --image gcr.io/${PROJECT_ID}/billing-staging-api:0.1 5 6gcloud run services list Task - 2 : Deploy the Frontend Service 1cd ~/pet-theory/lab07/staging-frontend-billing 2 3gcloud builds submit --tag gcr.io/${PROJECT_ID}/frontend-staging:0.1 4gcloud run deploy public-billing-service --image gcr.io/${PROJECT_ID}/frontend-staging:0.1 5 6gcloud run services list Task - 3 : Deploy a Private Billing Service 1cd ~/pet-theory/lab07/staging-api-billing 2 3gcloud builds submit --tag gcr.io/${PROJECT_ID}/billing-staging-api:0.2 4gcloud run deploy public-billing-service --image gcr.io/${PROJECT_ID}/billing-staging-api:0.2 5 6gcloud run services list 7 8BILLING_URL=$(gcloud run services describe $BILLING_SERVICE \\ 9 --platform managed \\ 10 --region us-central1 \\ 11 --format \u0026#34;value(status.url)\u0026#34;) 12 13curl -X get -H \u0026#34;Authorization: Bearer $(gcloud auth print-identity-token)\u0026#34; $BILLING_URL Task - 4 : Create a Billing Service Account 1gcloud iam service-accounts create billing-service-sa --display-name \u0026#34;Billing Service Cloud Run\u0026#34; Task - 5 : Deploy a Billing Service in Production 1cd ~/pet-theory/lab07/prod-api-billing 2 3gcloud builds submit --tag gcr.io/${PROJECT_ID}/billing-prod-api:0.1 4gcloud run deploy public-billing-service --image gcr.io/${PROJECT_ID}/billing-prod-api:0.1 5 6gcloud run services list 7 8PROD_BILLING_SERVICE=private-billing-service 9 10PROD_BILLING_URL=$(gcloud run services \\ 11 describe $PROD_BILLING_SERVICE \\ 12 --platform managed \\ 13 --region us-central1 \\ 14 --format \u0026#34;value(status.url)\u0026#34;) 15 16curl -X get -H \u0026#34;Authorization: Bearer \\ 17$(gcloud auth print-identity-token)\u0026#34; \\ 18 $PROD_BILLING_URL Task - 6 : Create a Frontend Service Account 1gcloud iam service-accounts create frontend-service-sa --display-name \u0026#34;Billing Service Cloud Run Invoker\u0026#34; Task - 7 : Deploy the Frontend Service in Production 1cd ~/pet-theory/lab07/prod-frontend-billing 2 3gcloud builds submit --tag gcr.io/${PROJECT_ID}/frontend-prod:0.1 4gcloud run deploy public-billing-service --image gcr.io/${PROJECT_ID}/frontend-prod:0.1 5 6gcloud run services list Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp328-serverless-cloud-run-development/","title":"GSP-328: Serverless Cloud Run Development"},{"content":"Please use your own credentials while completing the lab -- that means using your own service account, database credentials, etc. wherever necessary.\nStart by executing the following commands:\n1gsutil cp gs://spls/gsp335/gsp335.zip . 1unzip gsp335.zip Task - 1: Setup cluster 1gcloud container clusters create \u0026lt;cluster-name\u0026gt; \\ 2 --zone us-central1-c \\ 3 --machine-type n1-standard-4 \\ 4 --num-nodes 2 \\ 5 --enable-network-policy Create the Cloud SQL instance:\n1gcloud sql instances create \u0026lt;your-sql-instance-name\u0026gt; --region us-central1 Task - 2: Setup wordpress 1gcloud iam service-accounts create \u0026lt;your-service-account-credentials\u0026gt; 2 3gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \\ 4 --member=\u0026#34;serviceAccount:\u0026lt;your-service-account-credentials\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ 5 --role=\u0026#34;roles/cloudsql.client\u0026#34; 6 7gcloud iam service-accounts keys create key.json --iam-account=\u0026lt;your-service-account-credentials\u0026gt;@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com 8 9kubectl create secret generic cloudsql-instance-credentials --from-file key.json 10 11kubectl create secret generic cloudsql-db-credentials \\ 12 --from-literal username=wordpress \\ 13 --from-literal password=\u0026#39;\u0026#39; Remember the passowrd you set-up above as you'll need it later.\nCreate the WordPress deployment and service\n1kubectl create -f volume.yaml Go to the editor and replace instance name with SQL instance name.\nGo to the overview page of your Cloud SQL instance, and copy the Connection name.\nOpen wordpress.yaml with your any editor, and replace INSTANCE_CONNECTION_NAME (in line 61) with the Connection name of your Cloud SQL instance and Save the file changes.\n1kubectl apply -f wordpress.yaml Task - 3: Setup Ingress with TLS 1helm version 2 3helm repo add stable https://charts.helm.sh/stable 4helm repo update  If your environment does not install with Helm  1curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 2chmod 700 get_helm.sh 3./get_helm.sh  Now, you can continue:  1helm install nginx-ingress stable/nginx-ingress --set rbac.create=true 2 3kubectl get service nginx-ingress-controller 4 5. add_ip.sh 6 7kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.16.0/cert-manager.yaml 8 9kubectl create clusterrolebinding cluster-admin-binding \\ 10 --clusterrole=cluster-admin \\ 11 --user=$(gcloud config get-value core/account)  Edit issuer.yaml and set the email address Save the file changes and run  1kubectl apply -f issuer.yaml   Edit ingress.yaml and set your YOUR_LAB_USERNAME.labdns.xyz DNS record to lines 11 and 14.\n  Save the file changes and run\n  1kubectl apply -f ingress.yaml Task - 4: Set up Network Policy 1nano network-policy.yaml  Set the values of name and spec as shown below  1apiVersion:networking.k8s.io/v12kind:NetworkPolicy3metadata:4name:allow-nginx-access-to-internet5spec:6podSelector:7matchLabels:8app:nginx-ingress9policyTypes:10- Ingress11ingress:12- {} Save the file by ctrl + x -\u0026gt; y -\u0026gt; enter  1kubectl apply -f network-policy.yaml Task - 5: Setup Binary Authorization  Goto Cloud Console -\u0026gt; Security -\u0026gt; Binary Authorization. Enable the Binary Authorization API. On Binary Authorization page, click CONFIGURE POLICY. Select Disallow all images for the Default rule. Scroll down to Images exempt from this policy, click ADD IMAGE PATH and paste  1docker.io/library/wordpress:latest  Repeat the above two steps to add the following image paths  1us.gcr.io/k8s-artifacts-prod/ingress-nginx/* 2gcr.io/cloudsql-docker/* 3quay.io/jetstack/*   Click SAVE POLICY.\n  Navigate to Kubernetes Engine -\u0026gt; Clusters.\n  Click your cluster name to view its detail page.\n  Edit Binary authorization and Enable Binary Authorization then SAVE CHANGES.\n  Task - 6: Setup Pod Security Policy In the video, editing for psp-restrictive.yaml is shown through script editor. For this tutorial, we are using nano.\n1nano psp-restrictive.yaml   replace appVersion: extensions/v1beta1 with policy/v1beta1\n  Save the changes \u0026amp; apply the config through kubectl.\n  1kubectl apply -f psp-role.yaml 2kubectl apply -f pop-use.yaml 3kubectl apply -f psp-restrictive.yaml Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp335-secure-workloads-in-google-kubernetes-engine/","title":"GSP-335 : Secure Workloads in Google Kubernetes Engine"},{"content":"Enter the following commands:\n1gsutil cp gs://sureskills-ql/challenge-labs/ch05-k8s-scale-and-update/echo-web-v2.tar.gz . 2 3tar xvzf echo-web-v2.tar.gz 4 5gcloud builds submit --tag gcr.io/$DEVSHELL_PROJECT_ID/echo-app:v2 . 6 7gcloud container clusters get-credentials echo-cluster --zone us-central1-a 8 9kubectl create deployment echo-web --image=gcr.io/qwiklabs-resources/echo-app:v1 10 11kubectl expose deployment echo-web --type=LoadBalancer --port 80 --target-port 8000 12 13kubectl edit deploy echo-web  Change image version 'v1' to 'v2' by pressing \u0026quot;i\u0026quot; Save by esc -\u0026gt; Esc - \u0026gt; \u0026quot;:wq\u0026quot; and press enter.  1kubectl scale deploy echo-web --replicas=2 Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp305-scale-out-and-update-a-containerized-application-on-a-kubernetes-cluster/","title":"GSP-305: Scale Out and Update a Containerized Application on a Kubernetes Cluster"},{"content":"Task - 1 : Run a simple Dataflow job 1bq mk lab 2 3gsutil cp gs://cloud-training/gsp323/lab.csv . 4 5cat lab.csv 6 7gsutil cp gs://cloud-training/gsp323/lab.schema . 8 9cat lab.schema Task - 2 : Run a simple Dataproc job  This has to be done mannually.  Task - 3 : Run a simple Dataprep job  This has to be done mannually.  Task - 4 : AI 1gcloud iam service-accounts create my-natlang-sa \\ 2 --display-name \u0026#34;my natural language service account\u0026#34; 3 4gcloud iam service-accounts keys create ~/key.json \\ 5 --iam-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com 6 7export GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;/home/$USER/key.json\u0026#34; 8 9gcloud auth activate-service-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --key-file=$GOOGLE_APPLICATION_CREDENTIALS 10 11gcloud ml language analyze-entities --content=\u0026#34;Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat.\u0026#34; \u0026gt; result.json 12 13gcloud auth login 14(Copy the token from the link provided) 15 16 17gsutil cp result.json gs://YOUR_PROJECT-marking/task4-cnl.result Create an API key and export as API_KEY variable. 1export API_KEY={Replace with API KEY} 2 3nano request.json Add this content:\n1{ 2 \u0026#34;config\u0026#34;: { 3 \u0026#34;encoding\u0026#34;:\u0026#34;FLAC\u0026#34;, 4 \u0026#34;languageCode\u0026#34;: \u0026#34;en-US\u0026#34; 5 }, 6 \u0026#34;audio\u0026#34;: { 7 \u0026#34;uri\u0026#34;:\u0026#34;gs://cloud-training/gsp323/task4.flac\u0026#34; 8 } 9} 1curl -s -X POST -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @request.json \\ 2\u0026#34;https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}\u0026#34; \u0026gt; result.json 3 4gsutil cp result.json gs://YOUR_PROJECT-marking/task4-gcs.result 5 6 7gcloud iam service-accounts create quickstart 8 9gcloud iam service-accounts keys create key.json --iam-account quickstart@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com 10 11gcloud auth activate-service-account --key-file key.json 12 13export ACCESS_TOKEN=$(gcloud auth print-access-token) 14 15 16nano request.json Add this content:\n1{ 2 \u0026#34;inputUri\u0026#34;:\u0026#34;gs://spls/gsp154/video/chicago.mp4\u0026#34;, 3 \u0026#34;features\u0026#34;: [ 4 \u0026#34;TEXT_DETECTION\u0026#34; 5 ] 6} Now add the following commands on the command line:\n1curl -s -H \u0026#39;Content-Type: application/json\u0026#39; \\ 2 -H \u0026#34;Authorization: Bearer $ACCESS_TOKEN\u0026#34; \\ 3 \u0026#39;https://videointelligence.googleapis.com/v1/videos:annotate\u0026#39; \\ 4 -d @request.json 5 6 7 8curl -s -H \u0026#39;Content-Type: application/json\u0026#39; -H \u0026#34;Authorization: Bearer $ACCESS_TOKEN\u0026#34; \u0026#39;https://videointelligence.googleapis.com/v1/operations/OPERATION_FROM_PREVIOUS_REQUEST\u0026#39; \u0026gt; result1.json 9 10 11gsutil cp result1.json gs://YOUR_PROJECT-marking/task4-gvi.result Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp323-perform-foundational-data-ml-and-ai-tasks-in-google-cloud/","title":"GSP-323: Perform Foundational Data, ML, and AI Tasks in Google Cloud"},{"content":"Task 1 : Create cluster and deploy an app 1ZONE=us-central1-a 2 3gcloud container clusters create onlineboutique-cluster \\ 4 --project=${DEVSHELL_PROJECT_ID} --zone=${ZONE} \\ 5 --machine-type=n1-standard-2 --num-nodes=2 6 7kubectl create namespace dev 8kubectl create namespace prod 9 10kubectl config set-context --current --namespace dev 11 12git clone https://github.com/GoogleCloudPlatform/microservices-demo.git 13cd microservices-demo 14kubectl apply -f ./release/kubernetes-manifests.yaml --namespace dev 15 16kubectl get svc -w --namespace dev  Open http://\u0026lt;EXTERNAL_IP\u0026gt; in a new tab. You should see the homepage of the Online Boutique application.  Task 2 : Migrate to an Optimized Nodepool 1gcloud container node-pools create optimized-pool \\ 2 --cluster=onlineboutique-cluster \\ 3 --machine-type=custom-2-3584 \\ 4 --num-nodes=2 \\ 5 --zone=$ZONE 6 7for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do 8 kubectl cordon \u0026#34;$node\u0026#34;; 9done 10 11for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do 12 kubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 \u0026#34;$node\u0026#34;; 13done 14 15kubectl get pods -o=wide --namespace dev 16 17gcloud container node-pools delete default-pool \\ 18 --cluster onlineboutique-cluster --zone $ZONE Task 3 : Apply a Frontend Update 1kubectl create poddisruptionbudget onlineboutique-frontend-pdb \\ 2--selector app=frontend --min-available 1 --namespace dev 3 4KUBE_EDITOR=\u0026#34;nano\u0026#34; kubectl edit deployment/frontend --namespace dev  Change the following in the configuration to  1 image to gcr.io/qwiklabs-resources/onlineboutique-frontend:v2.1 2 imagePullPolicy to Always Save by: ctrl + o -\u0026gt; Enter -\u0026gt; ctrl + x\nTask 4 : Autoscale from Estimated Traffic 1kubectl autoscale deployment frontend --cpu-percent=50 \\ 2 --min=1 --max=13 --namespace dev 3 4kubectl get hpa --namespace dev 5 6gcloud beta container clusters update onlineboutique-cluster \\ 7 --enable-autoscaling --min-nodes 1 --max-nodes 6 --zone $ZONE 8 9kubectl exec $(kubectl get pod --namespace=dev | grep \u0026#39;loadgenerator\u0026#39; | cut -f1 -d \u0026#39; \u0026#39;) \\ 10 -it --namespace=dev -- bash -c \u0026#34;export USERS=8000; sh ./loadgen.sh\u0026#34; Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp343-optimize-costs-for-google-kubernetes-engine/","title":"GSP-343: Optimize Costs for Google Kubernetes Engine"},{"content":"1export ZONE=us-central1-a 2 3gcloud sql instances create wordpress --tier=db-n1-standard-1 --activation-policy=ALWAYS --gce-zone $ZONE 4 5gcloud sql users set-password --host % root --instance wordpress --password Password1* 6 7export ADDRESS=\u0026lt;external IP of blog vm/32\u0026gt; 8 9gcloud sql instances patch wordpress --authorized-networks $ADDRESS --quiet 10 11gcloud compute ssh blog --zone=us-central1-a 12 13MYSQLIP=$(gcloud sql instances describe wordpress --format=\u0026#34;value(ipAddresses.ipAddress)\u0026#34;) 14 15mysql --host=$MYSQLIP \\ 16 --user=root --password 17 18CREATE DATABASE wordpress; 19CREATE USER \u0026#39;blogadmin\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Password1*\u0026#39;; 20GRANT ALL PRIVILEGES ON wordpress.* TO \u0026#39;blogadmin\u0026#39;@\u0026#39;%\u0026#39;; 21FLUSH PRIVILEGES; 22 23exit 24 25sudo mysqldump -u root -pPassword1* wordpress \u0026gt; wordpress_backup.sql 26 27mysql --host=$MYSQLIP --user=root -pPassword1* --verbose wordpress \u0026lt; wordpress_backup.sql 28 29sudo service apache2 restart 30 31cd /var/www/html/wordpress 32 33sudo nano wp-config.php Replace the localhost with SQL Instance IP Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp306-migrate-a-mysql-database-to-google-cloud-sql/","title":"GSP-306: Migrate a Mysql Database to Google Cloud Sql"},{"content":"Run in Cloud Shell\n1export SANAME=challenge 2gcloud iam service-accounts create $SANAME 3gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member=serviceAccount:$SANAME@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role=roles/bigquery.admin 4gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID --member=serviceAccount:$SANAME@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role=roles/storage.admin 5gcloud iam service-accounts keys create sa-key.json --iam-account $SANAME@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com 6export GOOGLE_APPLICATION_CREDENTIALS=${PWD}/sa-key.json 7gsutil cp gs://$DEVSHELL_PROJECT_ID/analyze-images.py .  Open Editor and replace the content of \u0026quot;analyze-images.py\u0026quot; file with  1# Dataset: image_classification_dataset 2 3# Table name: image_text_detail 4 5import os 6 7import sys 8 9 10 11# Import Google Cloud Library modules 12 13from google.cloud import storage, bigquery, language, vision, translate_v2 14 15 16 17if (\u0026#39;GOOGLE_APPLICATION_CREDENTIALS\u0026#39; in os.environ): 18 19 if (not os.path.exists(os.environ[\u0026#39;GOOGLE_APPLICATION_CREDENTIALS\u0026#39;])): 20 21 print (\u0026#34;The GOOGLE_APPLICATION_CREDENTIALS file does not exist.\\n\u0026#34;) 22 23 exit() 24 25else: 26 27 print (\u0026#34;The GOOGLE_APPLICATION_CREDENTIALS environment variable is not defined.\\n\u0026#34;) 28 29 exit() 30 31 32 33if len(sys.argv)\u0026lt;3: 34 35 print(\u0026#39;You must provide parameters for the Google Cloud project ID and Storage bucket\u0026#39;) 36 37 print (\u0026#39;python3 \u0026#39;+sys.argv[0]+ \u0026#39;[PROJECT_NAME] [BUCKET_NAME]\u0026#39;) 38 39 exit() 40 41 42 43project_name = sys.argv[1] 44 45bucket_name = sys.argv[2] 46 47# Set up our GCS, BigQuery, and Natural Language clients 48 49storage_client = storage.Client() 50 51bq_client = bigquery.Client(project=project_name) 52 53nl_client = language.LanguageServiceClient() 54 55 56 57# Set up client objects for the vision and translate_v2 API Libraries 58 59vision_client = vision.ImageAnnotatorClient() 60 61translate_client = translate_v2.Client() 62 63 64 65# Setup the BigQuery dataset and table objects 66 67dataset_ref = bq_client.dataset(\u0026#39;image_classification_dataset\u0026#39;) 68 69dataset = bigquery.Dataset(dataset_ref) 70 71table_ref = dataset.table(\u0026#39;image_text_detail\u0026#39;) 72 73table = bq_client.get_table(table_ref) 74 75 76 77# Create an array to store results data to be inserted into the BigQuery table 78 79rows_for_bq = [] 80 81 82 83# Get a list of the files in the Cloud Storage Bucket 84 85files = storage_client.bucket(bucket_name).list_blobs() 86 87bucket = storage_client.bucket(bucket_name) 88 89 90 91print(\u0026#39;Processing image files from GCS. This will take a few minutes..\u0026#39;) 92 93 94 95# Process files from Cloud Storage and save the result to send to BigQuery 96 97for file in files: 98 99 if file.name.endswith(\u0026#39;jpg\u0026#39;) or file.name.endswith(\u0026#39;png\u0026#39;): 100 101 file_content = file.download_as_string() 102 103 104 105 # TBD: Create a Vision API image object called image_object 106 107 # Ref: https://googleapis.dev/python/vision/latest/gapic/v1/types.html#google.cloud.vision_v1.types.Image 108 109 from google.cloud import vision_v1 110 111 import io 112 113 client = vision.ImageAnnotatorClient() 114 115 116 117 118 119 # TBD: Detect text in the image and save the response data into an object called response 120 121 # Ref: https://googleapis.dev/python/vision/latest/gapic/v1/api.html#google.cloud.vision_v1.ImageAnnotatorClient.document_text_detection 122 123 image = vision_v1.types.Image(content=file_content) 124 125 response = client.text_detection(image=image) 126 127 128 129 # Save the text content found by the vision API into a variable called text_data 130 131 text_data = response.text_annotations[0].description 132 133 134 135 # Save the text detection response data in \u0026lt;filename\u0026gt;.txt to cloud storage 136 137 file_name = file.name.split(\u0026#39;.\u0026#39;)[0] + \u0026#39;.txt\u0026#39; 138 139 blob = bucket.blob(file_name) 140 141 # Upload the contents of the text_data string variable to the Cloud Storage file 142 143 blob.upload_from_string(text_data, content_type=\u0026#39;text/plain\u0026#39;) 144 145 146 147 # Extract the description and locale data from the response file 148 149 # into variables called desc and locale 150 151 # using response object properties e.g. response.text_annotations[0].description 152 153 desc = response.text_annotations[0].description 154 155 locale = response.text_annotations[0].locale 156 157 158 159 # if the locale is English (en) save the description as the translated_txt 160 161 if locale == \u0026#39;en\u0026#39;: 162 163 translated_text = desc 164 165 else: 166 167 # TBD: For non EN locales pass the description data to the translation API 168 169 # ref: https://googleapis.dev/python/translation/latest/client.html#google.cloud.translate_v2.client.Client.translate 170 171 # Set the target_language locale to \u0026#39;en\u0026#39;) 172 173 from google.cloud import translate_v2 as translate 174 175 176 177 client = translate.Client() 178 179 translation = translate_client.translate(text_data, target_language=\u0026#39;en\u0026#39;) 180 181 translated_text = translation[\u0026#39;translatedText\u0026#39;] 182 183 print(translated_text) 184 185 186 187 # if there is response data save the original text read from the image, 188 189 # the locale, translated text, and filename 190 191 if len(response.text_annotations) \u0026gt; 0: 192 193 rows_for_bq.append((desc, locale, translated_text, file.name)) 194 195 196 197print(\u0026#39;Writing Vision API image data to BigQuery...\u0026#39;) 198 199# Write original text, locale and translated text to BQ 200 201# TBD: When the script is working uncomment the next line to upload results to BigQuery 202 203errors = bq_client.insert_rows(table, rows_for_bq) 204 205 206 207assert errors == [] 208  In Cloud Shell run  1python3 analyze-images.py $DEVSHELL_PROJECT_ID $DEVSHELL_PROJECT_ID  Navigation Menu -\u0026gt; BigQuery, Run  1SELECTlocale,COUNT(locale)aslcountFROMimage_classification_dataset.image_text_detailGROUPBYlocaleORDERBYlcountDESCCongratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp329-integrate-with-machine-learning-apis/","title":"GSP-329: Integrate With Machine Learning APIs"},{"content":"From Navigation Menu -\u0026gt; BigQuery.\nTask - 1 : Total Confirmed Cases 1SELECTsum(cumulative_confirmed)astotal_cases_worldwide2FROM`bigquery-public-data.covid19_open_data.covid19_open_data`3WHEREdate=\u0026#39;2020-04-15\u0026#39;Task - 2 : Worst Affected Areas 1withdeaths_by_statesas(23SELECTsubregion1_nameasstate,sum(cumulative_deceased)asdeath_count45FROM`bigquery-public-data.covid19_open_data.covid19_open_data`67wherecountry_name=\u0026#34;United States of America\u0026#34;anddate=\u0026#39;2020-04-10\u0026#39;andsubregion1_nameisNOTNULL89groupbysubregion1_name10)1112selectcount(*)ascount_of_states1314fromdeaths_by_states1516wheredeath_count\u0026gt;100Task - 3 : Identifying Hotspots 1SELECT*FROM(23SELECTsubregion1_nameasstate,sum(cumulative_confirmed)astotal_confirmed_cases45FROM`bigquery-public-data.covid19_open_data.covid19_open_data`67WHEREcountry_code=\u0026#34;US\u0026#34;ANDdate=\u0026#39;2020-04-10\u0026#39;ANDsubregion1_nameisNOTNULL89GROUPBYsubregion1_name1011ORDERBYtotal_confirmed_casesDESC12)13WHEREtotal_confirmed_cases\u0026gt;1000Task - 4 : Fatality Ratio 1SELECTsum(cumulative_confirmed)astotal_confirmed_cases,sum(cumulative_deceased)astotal_deaths,(sum(cumulative_deceased)/sum(cumulative_confirmed))*100ascase_fatality_ratio23FROM`bigquery-public-data.covid19_open_data.covid19_open_data`45wherecountry_name=\u0026#34;Italy\u0026#34;ANDdateBETWEEN\u0026#39;2020-04-01\u0026#39;and\u0026#39;2020-04-30\u0026#39;Task - 5 : Identifying specific day 1SELECTdate23FROM`bigquery-public-data.covid19_open_data.covid19_open_data`45wherecountry_name=\u0026#34;Italy\u0026#34;andcumulative_deceased\u0026gt;1000067orderbydateasc89limit1Task - 6 : Finding days with zero net new cases 1WITHindia_cases_by_dateAS(23SELECT45date,67SUM(cumulative_confirmed)AScases89FROM1011`bigquery-public-data.covid19_open_data.covid19_open_data`1213WHERE1415country_name=\u0026#34;India\u0026#34;1617ANDdatebetween\u0026#39;2020-02-21\u0026#39;and\u0026#39;2020-03-15\u0026#39;1819GROUPBY2021date2223ORDERBY2425dateASC2627)2829,india_previous_day_comparisonAS3031(SELECT3233date,3435cases,3637LAG(cases)OVER(ORDERBYdate)ASprevious_day,3839cases-LAG(cases)OVER(ORDERBYdate)ASnet_new_cases4041FROMindia_cases_by_date4243)4445selectcount(*)4647fromindia_previous_day_comparison4849wherenet_new_cases=0Task - 7 : Doubling rate 1WITHus_cases_by_dateAS(23SELECT45date,67SUM(cumulative_confirmed)AScases89FROM1011`bigquery-public-data.covid19_open_data.covid19_open_data`1213WHERE1415country_name=\u0026#34;United States of America\u0026#34;1617ANDdatebetween\u0026#39;2020-03-22\u0026#39;and\u0026#39;2020-04-20\u0026#39;1819GROUPBY2021date2223ORDERBY2425dateASC2627)28293031,us_previous_day_comparisonAS3233(SELECT3435date,3637cases,3839LAG(cases)OVER(ORDERBYdate)ASprevious_day,4041cases-LAG(cases)OVER(ORDERBYdate)ASnet_new_cases,4243(cases-LAG(cases)OVER(ORDERBYdate))*100/LAG(cases)OVER(ORDERBYdate)ASpercentage_increase4445FROMus_cases_by_date4647)48495051selectDate,casesasConfirmed_Cases_On_Day,previous_dayasConfirmed_Cases_Previous_Day,percentage_increaseasPercentage_Increase_In_Cases5253fromus_previous_day_comparison5455wherepercentage_increase\u0026gt;10Task - 8 : Recovery rate 1WITHcases_by_countryAS(23SELECT45country_nameAScountry,67sum(cumulative_confirmed)AScases,89sum(cumulative_recovered)ASrecovered_cases1011FROM1213bigquery-public-data.covid19_open_data.covid19_open_data1415WHERE1617date=\u0026#39;2020-05-10\u0026#39;1819GROUPBY2021country_name2223)24252627,recovered_rateAS2829(SELECT3031country,cases,recovered_cases,3233(recovered_cases*100)/casesASrecovery_rate3435FROMcases_by_country3637)38394041SELECTcountry,casesASconfirmed_cases,recovered_cases,recovery_rate4243FROMrecovered_rate4445WHEREcases\u0026gt;500004647ORDERBYrecovery_ratedesc4849LIMIT10Task - 9 : CDGR - Cumulative Daily Growth Rate 1WITH23france_casesAS(45SELECT67date,89SUM(cumulative_confirmed)AStotal_cases1011FROM1213`bigquery-public-data.covid19_open_data.covid19_open_data`1415WHERE1617country_name=\u0026#34;France\u0026#34;1819ANDdateIN(\u0026#39;2020-01-24\u0026#39;,2021\u0026#39;2020-05-10\u0026#39;)2223GROUPBY2425date2627ORDERBY2829date)3031,summaryas(3233SELECT3435total_casesASfirst_day_cases,3637LEAD(total_cases)OVER(ORDERBYdate)ASlast_day_cases,3839DATE_DIFF(LEAD(date)OVER(ORDERBYdate),date,day)ASdays_diff4041FROM4243france_cases4445LIMIT14647)4849selectfirst_day_cases,last_day_cases,days_diff,POW((last_day_cases/first_day_cases),(1/days_diff))-1ascdgr5051fromsummaryTask - 10 : Create a Datastudio report 1SELECT23date,SUM(cumulative_confirmed)AScountry_cases,45SUM(cumulative_deceased)AScountry_deaths67FROM89`bigquery-public-data.covid19_open_data.covid19_open_data`1011WHERE1213dateBETWEEN\u0026#39;2020-03-15\u0026#39;1415AND\u0026#39;2020-04-30\u0026#39;1617ANDcountry_name=\u0026#34;United States of America\u0026#34;1819GROUPBYdateCongratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp787-insights-from-data-with-bigquery-challenge-lab/","title":"GSP-787: Insights From Data With Bigquery Challenge Lab"},{"content":"1gcloud config set compute/zone us-east1-b 2 3git clone https://source.developers.google.com/p/$DEVSHELL_PROJECT_ID/r/sample-app 4 5gcloud container clusters get-credentials jenkins-cd 6 7kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) 8 9helm repo add stable https://kubernetes-charts.storage.googleapis.com/ → this might not work… use the next line of code instead… 10 11helm repo add stable https://charts.helm.sh/stable 12 13helm repo update 14 15helm install cd stable/jenkins 16 17kubectl get pods 18 19export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/component=jenkins-master\u0026#34; -l \u0026#34;app.kubernetes.io/instance=cd\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) 20kubectl port-forward $POD_NAME 8080:8080 \u0026gt;\u0026gt; /dev/null \u0026amp; 21printf $(kubectl get secret cd-jenkins -o jsonpath=\u0026#34;{.data.jenkins-admin-password}\u0026#34; | base64 --decode);echo 1cd sample-app 2kubectl create ns production 3kubectl apply -f k8s/production -n production 4kubectl apply -f k8s/canary -n production 5kubectl apply -f k8s/services -n production 6 7kubectl get svc 8kubectl get service gceme-frontend -n production Branch Sources: Git  Project Repository: https://source.developers.google.com/p/[PROJECT_ID]/r/sample-app Credentials: qwiklabs service account  click Periodically if not...... 1 min\n1git init 2git config credential.helper gcloud.sh 3git remote add origin https://source.developers.google.com/p/$DEVSHELL_PROJECT_ID/r/sample-app 4git config --global user.email \u0026#34;\u0026lt;user email\u0026gt;\u0026#34; 5git config --global user.name \u0026#34;\u0026lt;user name\u0026gt;\u0026#34; 6git add . 7git commit -m \u0026#34;initial commit\u0026#34; 8git push origin master Now save your work.\n1git checkout -b new-feature 2 3git add Jenkinsfile html.go main.go 4git commit -m \u0026#34;Version 2.0.0\u0026#34; 5git push origin new-feature 6 7curl http://localhost:8001/api/v1/namespaces/new-feature/services/gceme-frontend:80/proxy/version 8kubectl get service gceme-frontend -n production 9git checkout -b canary 10git push origin canary 11export FRONTEND_SERVICE_IP=$(kubectl get -o \\ 12jsonpath=\u0026#34;{.status.loadBalancer.ingress[0].ip}\u0026#34; --namespace=production services gceme-frontend) 13git checkout master 14git push origin master 15 16 17export FRONTEND_SERVICE_IP=$(kubectl get -o \\ 18jsonpath=\u0026#34;{.status.loadBalancer.ingress[0].ip}\u0026#34; --namespace=production services gceme-frontend) 19while true; do curl http://$FRONTEND_SERVICE_IP/version; sleep 1; done 20 21kubectl get service gceme-frontend -n production 22 23git merge canary 24git push origin master 25export FRONTEND_SERVICE_IP=$(kubectl get -o \\ 26jsonpath=\u0026#34;{.status.loadBalancer.ingress[0].ip}\u0026#34; --namespace=production services gceme-frontend) Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp330-implement-devops-in-google-cloud/","title":"GSP-330: Implement DevOps in Google Cloud"},{"content":"Task 1 : Create a Compute Engine instance, add necessary firewall rules\n Goto Compute Engine -\u0026gt; VM Instannces -\u0026gt; Create Instance.  1Name :\u0026lt;Given in the lab\u0026gt;2Zone :\u0026lt;Given in the lab\u0026gt;3Series :N14Boot Disk :Default56Tick :Allow HTTP Traffic7Allow HTTPS Traffic Click Create.  Task 2 : Configure Apache2 Web Server in your instance\n SSH to 'apache' instance and run  1sudo su - 2apt-get update 3apt-get install apache2 -y 4service --status-all Task 3 : Test your server\n Click on External IP of apache instance.  Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp101-google-cloud-essential-skills/","title":"GSP-101: Google Cloud Essential Skills"},{"content":"Task - 5.1 : Store the transaction hash of the large mystery transfer of 194993 BTC in the table 51 inside the lab dataset:\n1CREATEORREPLACETABLElab.51(transaction_hashSTRING)as2SELECTtransaction_idFROM`bigquery-public-data.bitcoin_blockchain.transactions`,UNNEST(outputs)asoutputs3whereoutputs.output_satoshis=19499300000000Task - 5.2 : Store the balance of the pizza purchase address in the table 52 inside the lab dataset:\n1-- SQL source from https://cloud.google.com/blog/product... 2CREATEORREPLACETABLElab.52(balanceNUMERIC)as3WITHdouble_entry_bookAS(4-- debits 5SELECT6array_to_string(inputs.addresses,\u0026#34;,\u0026#34;)asaddress7,-inputs.valueasvalue8FROM`bigquery-public-data.crypto_bitcoin.inputs`asinputs9UNIONALL10-- credits 11SELECT12array_to_string(outputs.addresses,\u0026#34;,\u0026#34;)asaddress13,outputs.valueasvalue14FROM`bigquery-public-data.crypto_bitcoin.outputs`asoutputs1516)17SELECT18sum(value)asbalance19FROMdouble_entry_book20whereaddress=\u0026#34;1XPTgDRhN8RFnzniWCddobD9iKZatrvH4\u0026#34;Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp665-exploring-the-public-cryptocurrency-datasets-available-in-bigquery/","title":"GSP-665: Exploring the Public Cryptocurrency Datasets Available in BigQuery"},{"content":"Please execute the following tasks:\nTask - 1: Clean your training data 1CREATEORREPLACETABLE2taxirides.taxi_training_dataAS3SELECT4(tolls_amount+fare_amount)ASfare_amount,5pickup_datetime,6pickup_longitudeASpickuplon,7pickup_latitudeASpickuplat,8dropoff_longitudeASdropofflon,9dropoff_latitudeASdropofflat,10passenger_countASpassengers,11FROM12taxirides.historical_taxi_rides_raw13WHERE14RAND()\u0026lt;0.00115ANDtrip_distance\u0026gt;016ANDfare_amount\u0026gt;=2.517ANDpickup_longitude\u0026gt;-7818ANDpickup_longitude\u0026lt;-7019ANDdropoff_longitude\u0026gt;-7820ANDdropoff_longitude\u0026lt;-7021ANDpickup_latitude\u0026gt;3722ANDpickup_latitude\u0026lt;4523ANDdropoff_latitude\u0026gt;3724ANDdropoff_latitude\u0026lt;4525ANDpassenger_count\u0026gt;0Task - 2: Create a BQML model called taxirides.fare_model 1CREATEORREPLACEMODELtaxirides.fare_model2TRANSFORM(3*EXCEPT(pickup_datetime)45,ST_Distance(ST_GeogPoint(pickuplon,pickuplat),ST_GeogPoint(dropofflon,dropofflat))ASeuclidean6,CAST(EXTRACT(DAYOFWEEKFROMpickup_datetime)ASSTRING)ASdayofweek7,CAST(EXTRACT(HOURFROMpickup_datetime)ASSTRING)AShourofday8)9OPTIONS(input_label_cols=[\u0026#39;fare_amount\u0026#39;],model_type=\u0026#39;linear_reg\u0026#39;)10AS1112SELECT*FROMtaxirides.taxi_training_dataTask - 3: Perform a batch prediction on new data 1CREATEORREPLACETABLEtaxirides.2015_fare_amount_predictions2AS3SELECT*FROMML.PREDICT(MODELtaxirides.fare_model,(4SELECT*FROMtaxirides.report_prediction_data)5)Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp327-engineer-data-in-google-cloud/","title":"GSP-327: Engineer Data in Google Cloud"},{"content":"Please enter the following commands:\nTask - 1 : Create a Docker image and store the Dockerfile 1gcloud auth list 2gsutil cat gs://cloud-training/gsp318/marking/setup_marking_v2.sh | bash 3gcloud source repos clone valkyrie-app 4cd valkyrie-app 5cat \u0026gt; Dockerfile \u0026lt;\u0026lt;EOF 6FROM golang:1.10 7WORKDIR /go/src/app 8COPY source . 9RUN go install -v 10ENTRYPOINT [\u0026#34;app\u0026#34;,\u0026#34;-single=true\u0026#34;,\u0026#34;-port=8080\u0026#34;] 11EOF 12docker build -t \u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt; . 13cd .. 14cd marking 15./step1_v2.sh Task - 2 : Test the created Docker image 1cd .. 2cd valkyrie-app 3docker run -p 8080:8080 \u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt; \u0026amp; 4cd .. 5cd marking 6./step2_v2.sh Task - 3 : Push the Docker image in the Google Container Repository 1cd .. 2cd valkyrie-app 3docker tag \u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt; gcr.io/$GOOGLE_CLOUD_PROJECT/\u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt; 4docker push gcr.io/$GOOGLE_CLOUD_PROJECT/\u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt; Task - 4 : Create and expose a deployment in Kubernetes 1sed -i s#IMAGE_HERE#gcr.io/$GOOGLE_CLOUD_PROJECT/\u0026lt;Docker Image\u0026gt;:\u0026lt;Tag Name\u0026gt;#g k8s/deployment.yaml 2gcloud container clusters get-credentials valkyrie-dev --zone us-east1-d 3kubectl create -f k8s/deployment.yaml 4kubectl create -f k8s/service.yaml Task - 5 : Update the deployment with a new version of valkyrie-app 1git merge origin/kurt-dev 2kubectl edit deployment valkyrie-dev 3### change replicas from 1 to \u0026lt;Replicas Count\u0026gt; 4### change \u0026lt;Tag Name\u0026gt; to \u0026lt;Updated Version\u0026gt; in two places 5docker build -t gcr.io/$GOOGLE_CLOUD_PROJECT/\u0026lt;Docker Image\u0026gt;:\u0026lt;Updated Version\u0026gt; . 6docker push gcr.io/$GOOGLE_CLOUD_PROJECT/\u0026lt;Docker Image\u0026gt;:\u0026lt;Updated Version\u0026gt; Task - 6 : Create a pipeline in Jenkins to deploy your app 1docker ps 2docker kill \u0026lt;take container_id from above command\u0026gt; 3 4export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/component=jenkins-master\u0026#34; -l \u0026#34;app.kubernetes.io/instance=cd\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) 5kubectl port-forward $POD_NAME 8080:8080 \u0026gt;\u0026gt; /dev/null \u0026amp; 6printf $(kubectl get secret cd-jenkins -o jsonpath=\u0026#34;{.data.jenkins-admin-password}\u0026#34; | base64 --decode);echo  Open Jenkins Web View -\u0026gt; Preview on port 8080  1Username :admin2Password :{Code output from previous command} 3Go through the following:\n-\u0026gt; Manage Jenkins -\u0026gt; Manage Credentials -\u0026gt; Jenkins -\u0026gt; Global credentials (unrestricted) -\u0026gt; Add credentials -\u0026gt; Kind: Google Service Account from metadata -\u0026gt; OK\n-\u0026gt; Jenkins -\u0026gt; New Item -\u0026gt; Name : valkyrie-app -\u0026gt; Pipeline -\u0026gt; Pipeline script from SCM -\u0026gt; Set SCM to git -\u0026gt; OK\n-\u0026gt; Pipeline -\u0026gt; Script: Pipeline script from SCM -\u0026gt; SCM: Git\n-\u0026gt; Repository URL: {find it using command: gcloud source repos list} -\u0026gt; Credentials: {Project id}\n-\u0026gt; Apply -\u0026gt; Save\n1sed -i \u0026#34;s/green/orange/g\u0026#34; source/html.go 2 3sed -i \u0026#34;s/YOUR_PROJECT/$GOOGLE_CLOUD_PROJECT/g\u0026#34; Jenkinsfile 4git config --global user.email \u0026#34;you@example.com\u0026#34; // Email 5git config --global user.name \u0026#34;student...\u0026#34; // Username 6git add . 7git commit -m \u0026#34;built pipeline init\u0026#34; 8git push  In Jenkins click Build and wait to get score.  Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp318-deploy-to-kubernetes-in-google-cloud/","title":"GSP-318: Deploy to Kubernetes in Google Cloud"},{"content":"Task 1 : Create Production Environment  SSH to kraken-jumphost and run  1cd /work/dm 2sed -i s/SET_REGION/us-east1/g prod-network.yaml 3 4gcloud deployment-manager deployments create prod-network --config=prod-network.yaml 5 6gcloud config set compute/zone us-east1-b 7 8gcloud container clusters create kraken-prod \\ 9 --num-nodes 2 \\ 10 --network kraken-prod-vpc \\ 11 --subnetwork kraken-prod-subnet\\ 12 --zone us-east1-b 13 14gcloud container clusters get-credentials kraken-prod 15 16cd /work/k8s 17 18for F in $(ls *.yaml); do kubectl create -f $F; done Task 2 : Setup the Admin instance  Still in kraken-jumphost's SSH, run  1gcloud config set compute/zone us-east1-b 2 3gcloud compute instances create kraken-admin --network-interface=\u0026#34;subnet=kraken-mgmt-subnet\u0026#34; --network-interface=\u0026#34;subnet=kraken-prod-subnet\u0026#34; Create alert  Open monitoring Create an alert Configure the policy to email your email and set  1Resource Type :VM Instance2Metric :CPU utilization3Filter :instance_name4Value :kraken-admin5Condition :is above6Threshold :50%7For :1minuteTask 3 : Verify the Spinnaker deployment  Switch to cloudshell, run  1gcloud config set compute/zone us-east1-b 2 3gcloud container clusters get-credentials spinnaker-tutorial 4 5DECK_POD=$(kubectl get pods --namespace default -l \u0026#34;cluster=spin-deck\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) 6 7kubectl port-forward --namespace default $DECK_POD 8080:9000 \u0026gt;\u0026gt; /dev/null \u0026amp;   Go to cloudshell webpreview\n  Go to applications -\u0026gt; sample\n  Open pipelines and manually run the pipeline if it has not already running.\n  Approve the deployment to production.\n  Check the production frontend endpoint (use http, not the default https)\n  Back to cloudshell, run to push a change\n  1gcloud config set compute/zone us-east1-b 2 3gcloud source repos clone sample-app 4 5cd sample-app 6touch a 7 8git config --global user.email \u0026#34;$(gcloud config get-value account)\u0026#34; 9git config --global user.name \u0026#34;Student\u0026#34; 10git commit -a -m \u0026#34;change\u0026#34; 11git tag v1.0.1 12git push --tags Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp314-deploy-and-manage-cloud-environments-with-google-cloud/","title":"GSP-314 : Deploy and Manage Cloud Environments with Google Cloud"},{"content":"Please follow these commands:\nTask 1: Create a dataset to store your machine learning models In Cloud Shell\n1bq mk austin Navigation Menu -\u0026gt; BigQuery.\nTask 2: Create a forecasting BigQuery machine learning model In BigQuery Console Query Editor\n1CREATEORREPLACEMODELaustin.location_model2OPTIONS3(model_type=\u0026#39;linear_reg\u0026#39;,labels=[\u0026#39;duration_minutes\u0026#39;])AS4SELECT5start_station_name,6EXTRACT(HOURFROMstart_time)ASstart_hour,7EXTRACT(DAYOFWEEKFROMstart_time)ASday_of_week,8duration_minutes,9FROM10`bigquery-public-data.austin_bikeshare.bikeshare_trips`AStrips11JOIN12`bigquery-public-data.austin_bikeshare.bikeshare_stations`ASstations13ON14trips.start_station_name=stations.name15WHERE16EXTRACT(YEARFROMstart_time)=201817ANDduration_minutes\u0026gt;0Task 3: Create the second machine learning model In BigQuery Console Query Editor\n1CREATEORREPLACEMODELaustin.subscriber_model2OPTIONS3(model_type=\u0026#39;linear_reg\u0026#39;,labels=[\u0026#39;duration_minutes\u0026#39;])AS4SELECT5start_station_name,6EXTRACT(HOURFROMstart_time)ASstart_hour,7subscriber_type,8duration_minutes9FROM`bigquery-public-data.austin_bikeshare.bikeshare_trips`AStrips10WHEREEXTRACT(YEARFROMstart_time)=2018Task 4: Evaluate the two machine learning models In BigQuery Console Query Editor\nQuery - 1 1-- Evaluation metrics for location_model 2SELECT3SQRT(mean_squared_error)ASrmse,4mean_absolute_error5FROM6ML.EVALUATE(MODELaustin.location_model,(7SELECT8start_station_name,9EXTRACT(HOURFROMstart_time)ASstart_hour,10EXTRACT(DAYOFWEEKFROMstart_time)ASday_of_week,11duration_minutes12FROM13`bigquery-public-data.austin_bikeshare.bikeshare_trips`AStrips14JOIN15`bigquery-public-data.austin_bikeshare.bikeshare_stations`ASstations16ON17trips.start_station_name=stations.name18WHEREEXTRACT(YEARFROMstart_time)=2019)19)Query - 2 1-- Evaluation metrics for subscriber_model 2SELECT3SQRT(mean_squared_error)ASrmse,4mean_absolute_error5FROM6ML.EVALUATE(MODELaustin.subscriber_model,(7SELECT8start_station_name,9EXTRACT(HOURFROMstart_time)ASstart_hour,10subscriber_type,11duration_minutes12FROM13`bigquery-public-data.austin_bikeshare.bikeshare_trips`AStrips14WHERE15EXTRACT(YEARFROMstart_time)=2019)16)Task 5: Use the subscriber type machine learning model to predict average trip durations In BigQuery Console Query Editor\nQuery - 1 1SELECT2start_station_name,3COUNT(*)AStrips4FROM5`bigquery-public-data.austin_bikeshare.bikeshare_trips`6WHERE7EXTRACT(YEARFROMstart_time)=20198GROUPBY9start_station_name10ORDERBY11tripsDESCQuery - 2 1SELECTAVG(predicted_duration_minutes)ASaverage_predicted_trip_length2FROMML.predict(MODELaustin.subscriber_model,(3SELECT4start_station_name,5EXTRACT(HOURFROMstart_time)ASstart_hour,6subscriber_type,7duration_minutes8FROM9`bigquery-public-data.austin_bikeshare.bikeshare_trips`10WHERE11EXTRACT(YEARFROMstart_time)=201912ANDsubscriber_type=\u0026#39;Single Trip\u0026#39;13ANDstart_station_name=\u0026#39;21st \u0026amp; Speedway @PCL\u0026#39;))Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp341-create-ml-models-with-bigquery-ml/","title":"GSP-341: Create ML Models with BigQuery ML"},{"content":"Please follow these commands:\nTask 1 : Create a project jumphost instance Please navigate to the compute engine \u0026gt; VM instance \u0026amp; click on create instance. Instance Name: As mentioned in the lab instruction Machine type: N1 (f1-macro) Do not change the other settings just click on the create.\nTask 2 : Create a Kubernetes service cluster 1gcloud container clusters create nucleus-backend \\ 2 --num-nodes 1 \\ 3 --network nucleus-vpc \\ 4 --region us-east1 5gcloud container clusters get-credentials nucleus-backend \\ 6 --region us-east1 7 8kubectl create deployment hello-server \\ 9 --image=gcr.io/google-samples/hello-app:2.0 10 11kubectl expose deployment hello-server \\ 12 --type=LoadBalancer \\ 13 --port \u0026lt;Use port given in the lab\u0026gt; Task 3 : Set up an HTTP load balancer 1cat \u0026lt;\u0026lt; EOF \u0026gt; startup.sh 2#! /bin/bash 3apt-get update 4apt-get install -y nginx 5service nginx start 6sed -i -- \u0026#39;s/nginx/Google Cloud Platform - \u0026#39;\u0026#34;\\$HOSTNAME\u0026#34;\u0026#39;/\u0026#39; /var/www/html/index.nginx-debian.html 7EOF 1gcloud compute instance-templates create web-server-template \\ 2 --metadata-from-file startup-script=startup.sh \\ 3 --network nucleus-vpc \\ 4 --machine-type g1-small \\ 5 --region us-east1 6 7gcloud compute target-pools create nginx-pool 8 9gcloud compute instance-groups managed create web-server-group \\ 10 --base-instance-name web-server \\ 11 --size 2 \\ 12 --template web-server-template \\ 13 --region us-east1 14 15gcloud compute firewall-rules create \u0026lt;Copy FIREWALL_NAME given in the lab\u0026gt; \\ 16 --allow tcp:80 \\ 17 --network nucleus-vpc 18 19gcloud compute http-health-checks create http-basic-check 20 21gcloud compute instance-groups managed \\ 22 set-named-ports web-server-group \\ 23 --named-ports http:80 \\ 24 --region us-east1 25 26gcloud compute backend-services create web-server-backend \\ 27 --protocol HTTP \\ 28 --http-health-checks http-basic-check \\ 29 --global 30 31gcloud compute backend-services add-backend web-server-backend \\ 32 --instance-group web-server-group \\ 33 --instance-group-region us-east1 \\ 34 --global 35 36gcloud compute url-maps create web-server-map \\ 37 --default-service web-server-backend 38 39gcloud compute target-http-proxies create http-lb-proxy \\ 40 --url-map web-server-map 41 42gcloud compute forwarding-rules create permit-tcp-rule-261 \\ 43 --global \\ 44 --target-http-proxy http-lb-proxy \\ 45 --ports 80 1gcloud compute forwarding-rules list (Note: After running all the commands it can take upto 2-3 minutes to update the score for task 3) Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp313-create-and-manage-cloud-resources/","title":"GSP-313: Create and Manage Cloud Resources"},{"content":"Do the following steps:\n1mkdir deployment_manager 2cd deployment_manager 3gsutil cp gs://spls/gsp302/* . 4 5nano qwiklabs.jinja Delete all the content of qwiklabs.jinja and paste\n1resources: 2- type: compute.v1.instance 3 name: vm-test 4 properties: 5 zone: {{ properties[\u0026#34;zone\u0026#34;] }} 6 machineType: https://www.googleapis.com/compute/v1/projects/{{ env[\u0026#34;project\u0026#34;] }}/zones/{{ properties[\u0026#34;zone\u0026#34;] }}/machineTypes/f1-micro 7# For examples on how to use startup scripts on an instance, see: 8# https://cloud.google.com/compute/docs/startupscript 9 disks: 10 - deviceName: boot 11 type: PERSISTENT 12 boot: true 13 autoDelete: true 14 initializeParams: 15 diskName: disk-{{ env[\u0026#34;deployment\u0026#34;] }} 16 sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/family/debian-9 17 networkInterfaces: 18 - network: https://www.googleapis.com/compute/v1/projects/{{ env[\u0026#34;project\u0026#34;] }}/global/networks/default 19# Access Config required to give the instance a public IP address 20 accessConfigs: 21 - name: External NAT 22 type: ONE_TO_ONE_NAT 23 tags: 24 items: 25 - http 26 metadata: 27 items: 28 - key: startup-script 29 value: |30#!/bin/bash 31 apt-get update 32 apt-get install -y apache2 33- type: compute.v1.firewall 34 name: default-allow-http 35 properties: 36 network: https://www.googleapis.com/compute/v1/projects/{{ env[\u0026#34;project\u0026#34;] }}/global/networks/default 37 targetTags: 38 - http 39 allowed: 40 - IPProtocol: tcp 41 ports: 42 - \u0026#39;80\u0026#39; 43 sourceRanges: 44 - 0.0.0.0/0 To save press, ctrl + o -\u0026gt; Enter -\u0026gt; ctrl + x\n1nano qwiklabs.yaml Delete the content of qwiklabs.yaml and paste\n1imports: 2- path: qwiklabs.jinja 3 4resources: 5- name: qwiklabs 6 type: qwiklabs.jinja 7 properties: 8 zone: us-central1-a 1gcloud deployment-manager deployments create test --config=qwiklabs.yaml Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp302-configure-a-firewall-and-a-startup-script-with-deployment-manager/","title":"GSP-302 : Configure a Firewall and a Startup Script with Deployment Manager"},{"content":"Task 1 : Remove the overly permissive rules 1gcloud compute firewall-rules delete open-access Task 2 : Start the bastion host instance Go to Compute Engine and start Bastion instance.\nTask 3 : Create a firewall rule that allows SSH (tcp/22) from the IAP service and add network tag on bastion Replace the \u0026quot;[NETWORK TAG]\u0026quot; with the network tag provided in the lab.\n1gcloud compute firewall-rules create ssh-ingress --allow=tcp:22 --source-ranges 35.235.240.0/20 --target-tags [NETWORK TAG-1] --network acme-vpc 2 3gcloud compute instances add-tags bastion --tags=[NETWORK TAG-1] --zone=us-central1-b Task 4 : Create a firewall rule that allows traffic on HTTP (tcp/80) to any address and add network tag on juice-shop 1gcloud compute firewall-rules create http-ingress --allow=tcp:80 --source-ranges 0.0.0.0/0 --target-tags [NETWORK TAG-2] --network acme-vpc 2 3gcloud compute instances add-tags juice-shop --tags=[NETWORK TAG-2] --zone=us-central1-b Task 5 : Create a firewall rule that allows traffic on SSH (tcp/22) from acme-mgmt-subnet network address and add network tag on juice-shop 1gcloud compute firewall-rules create internal-ssh-ingress --allow=tcp:22 --source-ranges 192[dot]168[dot]10[dot]0/24 --target-tags [NETWORK TAG-3] --network acme-vpc 2 3gcloud compute instances add-tags juice-shop --tags=[NETWORK TAG-3] --zone=us-central1-b Task 6 : SSH to bastion host via IAP and juice-shop via bastion In Compute Engine -\u0026gt; VM Instances page, click the SSH button for the bastion host. Then SSH to juice-shop by\n1ssh [Internal IP address of juice-shop] Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp322-build-and-secure-networks-in-google-cloud/","title":"GSP-322 : Build and Secure Networks in Google Cloud: Challenge Lab"},{"content":"Create a dataset 1Dataset ID : ecommerce Task - 1 : Create a table partitioned by date 1CREATEORREPLACETABLEecommerce.sample2PARTITIONBYdate3OPTIONS(partition_expiration_days=90,4description=\u0026#34;COVID 19 data\u0026#34;5)AS6SELECT*7FROMbigquery-public-data.covid19_govt_response.oxford_policy_tracker8WHEREalpha_3_code!=\u0026#39;USA\u0026#39;ANDalpha_3_code!=\u0026#39;GBR\u0026#39;ANDalpha_3_code!=\u0026#39;BRA\u0026#39;ANDalpha_3_code!=\u0026#39;CAN\u0026#39;Task - 2 : Add new columns to your table 1ALTERTABLEecommerce.sample2ADDCOLUMNIFNOTEXISTSpopulationINT64,3ADDCOLUMNIFNOTEXISTScountry_areaFLOAT64,4ADDCOLUMNIFNOTEXISTSmobilitySTRUCT\u0026lt;5avg_retailFLOAT64,6avg_groceryFLOAT64,7avg_parksFLOAT64,8avg_transitFLOAT64,9avg_workplaceFLOAT64,10avg_residentialFLOAT6411\u0026gt;Task - 3 : Add country population data to the population column 1UPDATE`\u0026lt;PROJECT_ID\u0026gt;.ecommerce.sample`count2SETcount.population=count1.pop_data_20193FROM`bigquery-public-data.covid19_ecdc.covid_19_geographic_distribution_worldwide`count14WHEREcount.date=count1.dateANDcount.alpha_3_code=count1.country_territory_codeTask - 4 : Add country area data to the country_area column 1UPDATE`\u0026lt;PROJECT_ID\u0026gt;.ecommerce.sample`count2SETcount.country_area=count1.country_area3FROM`bigquery-public-data.census_bureau_international.country_names_area`count14WHEREcount.country_name=count1.country_nameTask - 5 : Populate the mobility record data 1UPDATE`\u0026lt;PROJECT_ID\u0026gt;.ecommerce.sample`count2SETcount.mobility=STRUCT\u0026lt;3avg_retailFLOAT64,avg_groceryFLOAT64,avg_parksFLOAT64,avg_transitFLOAT64,avg_workplaceFLOAT64,avg_residentialFLOAT644\u0026gt;5(count1.avg_retail,count1.avg_grocery,count1.avg_parks,count1.avg_transit,count1.avg_workplace,count1.avg_residential)6FROM(SELECTcountry_region,date,7AVG(retail_and_recreation_percent_change_from_baseline)asavg_retail,8AVG(grocery_and_pharmacy_percent_change_from_baseline)asavg_grocery,9AVG(parks_percent_change_from_baseline)asavg_parks,10AVG(transit_stations_percent_change_from_baseline)asavg_transit,11AVG(workplaces_percent_change_from_baseline)asavg_workplace,12AVG(residential_percent_change_from_baseline)asavg_residential13FROM`bigquery-public-data.covid19_google_mobility.mobility_report`14GROUPBYcountry_region,date)AScount115WHEREcount.country_name=count1.country_region16ANDcount.date=count1.dateTask - 6 : Query missing data in population \u0026amp; country_area columns 1SELECTDISTINCTcountry_name2FROM`\u0026lt;PROJECT_ID\u0026gt;.ecommerce.sample`3WHEREpopulationisNULL4UNIONALL5SELECTDISTINCTcountry_name6FROM`\u0026lt;PROJECT_ID\u0026gt;.ecommerce.sample`7WHEREcountry_areaISNULL8ORDERBYcountry_nameASCCongratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp340-build-and-optimize-data-warehouses-with-bigquery/","title":"GSP-340 : Build and Optimize Data Warehouses with BigQuery: Challenge Lab"},{"content":"Task 1: Download the monolith code and build your container 1git clone https://github.com/googlecodelabs/monolith-to-microservices.git 2 3cd ~/monolith-to-microservices 4./setup.sh 5 6cd ~/monolith-to-microservices/monolith 7npm start 8 9gcloud services enable cloudbuild.googleapis.com 10gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/fancytest:1.0.0 . Task 2: Create a kubernetes cluster and deploy the application 1gcloud config set compute/zone us-central1-a 2gcloud services enable container.googleapis.com 3gcloud container clusters create fancy-cluster --num-nodes 3 4 5kubectl create deployment fancytest --image=gcr.io/${GOOGLE_CLOUD_PROJECT}/fancytest:1.0.0 6kubectl expose deployment fancytest --type=LoadBalancer --port 80 --target-port 8080 Task 3: Create a containerized version of your Microservices 1cd ~/monolith-to-microservices/microservices/src/orders 2gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/orders:1.0.0 . 3 4cd ~/monolith-to-microservices/microservices/src/products 5gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/products:1.0.0 . Task 4: Deploy the new microservices 1kubectl create deployment orders --image=gcr.io/${GOOGLE_CLOUD_PROJECT}/orders:1.0.0 2kubectl expose deployment orders --type=LoadBalancer --port 80 --target-port 8081 3 4kubectl create deployment products --image=gcr.io/${GOOGLE_CLOUD_PROJECT}/products:1.0.0 5kubectl expose deployment products --type=LoadBalancer --port 80 --target-port 8082 Task 5: Configure the Frontend microservice 1cd ~/monolith-to-microservices/react-app 2nano .env Task 6: Create a containerized version of the Frontend microservice 1cd ~/monolith-to-microservices/microservices/src/frontend 2gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/frontend:1.0.0 . Task 7: Deploy the Frontend microservice 1kubectl create deployment frontend --image=gcr.io/${GOOGLE_CLOUD_PROJECT}/frontend:1.0.0 2 3kubectl expose deployment frontend --type=LoadBalancer --port 80 --target-port 8080 Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp319-build-a-website-on-google-cloud/","title":"GSP-319 : Build a Website on Google Cloud: Challenge Lab"},{"content":"Setup : Create the configuration files Make the empty files and directories in Cloud Shell or the Cloud Shell Editor.\n1touch main.tf 2touch variables.tf 3mkdir modules 4cd modules 5mkdir instances 6cd instances 7touch instances.tf 8touch outputs.tf 9touch variables.tf 10cd .. 11mkdir storage 12cd storage 13touch storage.tf 14touch outputs.tf 15touch variables.tf 16cd Add the following to the each variables.tf file, and fill in the GCP Project ID:\n1variable \u0026#34;region\u0026#34; { 2 default = \u0026#34;us-central1\u0026#34; 3} 45variable \u0026#34;zone\u0026#34; { 6 default = \u0026#34;us-central1-a\u0026#34; 7} 89variable \u0026#34;project_id\u0026#34; { 10 default = \u0026#34;\u0026lt;FILL IN PROJECT ID\u0026gt;\u0026#34; 11} Add the following to the main.tf file :\n1terraform { 2 required_providers { 3 google = { 4 source = \u0026#34;hashicorp/google\u0026#34; 5 version = \u0026#34;3.55.0\u0026#34; 6 } 7 } 8} 910provider \u0026#34;google\u0026#34; { 11 project = var.project_id 12 region = var.region 13 14 zone = var.zone 15} 1617module \u0026#34;instances\u0026#34; { 18 19 source = \u0026#34;./modules/instances\u0026#34; 20 21} Run terraform init in Cloud Shell in the root directory to initialize terraform.\nTask - 1 : Import infrastructure Navigate to Compute Engine \u0026gt; VM Instances. Click on tf-instance-1. Copy the Instance ID down somewhere to use later.\nNavigate to Compute Engine \u0026gt; VM Instances. Click on tf-instance-2. Copy the Instance ID down somewhere to use later.\nNext, navigate to modules/instances/instances.tf. Copy the following configuration into the file:\n1resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-1\u0026#34; { 2 name = \u0026#34;tf-instance-1\u0026#34; 3 machine_type = \u0026#34;n1-standard-1\u0026#34; 4 zone = var.zone 5 6 boot_disk { 7 initialize_params { 8 image = \u0026#34;debian-cloud/debian-10\u0026#34; 9 } 10 } 11 12 network_interface { 13 network = \u0026#34;default\u0026#34; 14 } 15} 1617resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-2\u0026#34; { 18 name = \u0026#34;tf-instance-2\u0026#34; 19 machine_type = \u0026#34;n1-standard-1\u0026#34; 20 zone = var.zone 21 22 boot_disk { 23 initialize_params { 24 image = \u0026#34;debian-cloud/debian-10\u0026#34; 25 } 26 } 27 28 network_interface { 29 network = \u0026#34;default\u0026#34; 30 } 31} To import the first instance, use the following command, using the Instance ID for tf-instance-1 you copied down earlier.\n1terraform import module.instances.google_compute_instance.tf-instance-1 \u0026lt;Instance ID - 1\u0026gt; To import the second instance, use the following command, using the Instance ID for tf-instance-2 you copied down earlier.\n1terraform import module.instances.google_compute_instance.tf-instance-2 \u0026lt;Instance ID - 2\u0026gt; The two instances have now been imported into your terraform configuration. You can now optionally run the commands to update the state of Terraform. Type yes at the dialogue after you run the apply command to accept the state changes.\n1terraform plan 2terraform apply Task - 2 : Configure a remote backend Add the following code to the modules/storage/storage.tf file:\n1resource \u0026#34;google_storage_bucket\u0026#34; \u0026#34;storage-bucket\u0026#34; { 2 name = var.project_id 3 location = \u0026#34;US\u0026#34; 4 force_destroy = true 5 uniform_bucket_level_access = true 6} Next, add the following to the main.tf file:\n1module \u0026#34;storage\u0026#34; { 2 source = \u0026#34;./modules/storage\u0026#34; 3} Run the following commands to initialize the module and create the storage bucket resource. Type yes at the dialogue after you run the apply command to accept the state changes.\n1terraform init 2terraform apply Next, update the main.tf file so that the terraform block looks like the following. Fill in your GCP Project ID for the bucket argument definition.\n1terraform { 2 backend \u0026#34;gcs\u0026#34; { 3 bucket = \u0026#34;\u0026lt;FILL IN PROJECT ID\u0026gt;\u0026#34; 4 prefix = \u0026#34;terraform/state\u0026#34; 5 } 6 required_providers { 7 google = { 8 source = \u0026#34;hashicorp/google\u0026#34; 9 version = \u0026#34;3.55.0\u0026#34; 10 } 11 } 12} Run the following to initialize the remote backend. Type yes at the prompt.\n1terraform init Task - 3 : Modify and update infrastructure Navigate to modules/instances/instance.tf. Replace the entire contents of the file with the following:\n1resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-1\u0026#34; { 2 name = \u0026#34;tf-instance-1\u0026#34; 3 machine_type = \u0026#34;n1-standard-2\u0026#34; 4 zone = var.zone 5 allow_stopping_for_update = true 6 7 boot_disk { 8 initialize_params { 9 image = \u0026#34;debian-cloud/debian-10\u0026#34; 10 } 11 } 12 13 network_interface { 14 network = \u0026#34;default\u0026#34; 15 } 16} 1718resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-2\u0026#34; { 19 name = \u0026#34;tf-instance-2\u0026#34; 20 machine_type = \u0026#34;n1-standard-2\u0026#34; 21 zone = var.zone 22 allow_stopping_for_update = true 23 24 boot_disk { 25 initialize_params { 26 image = \u0026#34;debian-cloud/debian-10\u0026#34; 27 } 28 } 29 30 network_interface { 31 network = \u0026#34;default\u0026#34; 32 } 33} 3435resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-3\u0026#34; { 36 name = \u0026#34;tf-instance-3\u0026#34; 37 machine_type = \u0026#34;n1-standard-2\u0026#34; 38 zone = var.zone 39 allow_stopping_for_update = true 40 41 boot_disk { 42 initialize_params { 43 image = \u0026#34;debian-cloud/debian-10\u0026#34; 44 } 45 } 46 47 network_interface { 48 network = \u0026#34;default\u0026#34; 49 } 50} Run the following commands to initialize the module and create/update the instance resources. Type yes at the dialogue after you run the apply command to accept the state changes.\n1terraform init 2terraform apply Task - 4 : Taint and destroy resources Taint the tf-instance-3 resource by running the following command:\n1terraform taint module.instances.google_compute_instance.tf-instance-3 Run the following commands to apply the changes:\n1terraform init 2terraform apply Remove the tf-instance-3 resource from the instances.tf file. Delete the following code chunk from the file.\n1resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-3\u0026#34; { 2 name = \u0026#34;tf-instance-3\u0026#34; 3 machine_type = \u0026#34;n1-standard-2\u0026#34; 4 zone = var.zone 5 allow_stopping_for_update = true 6 7 boot_disk { 8 initialize_params { 9 image = \u0026#34;debian-cloud/debian-10\u0026#34; 10 } 11 } 12 13 network_interface { 14 network = \u0026#34;default\u0026#34; 15 } 16} Run the following commands to apply the changes. Type yes at the prompt.\n1terraform apply Task - 5 : Use a module from the Registry Copy and paste the following into the main.tf file:\n1module \u0026#34;vpc\u0026#34; { 2 source = \u0026#34;terraform-google-modules/network/google\u0026#34; 3 version = \u0026#34;~\u0026gt; 3.2.2\u0026#34; 4 5 project_id = var.project_id 6 network_name = \u0026#34;terraform-vpc\u0026#34; 7 routing_mode = \u0026#34;GLOBAL\u0026#34; 8 9 subnets = [ 10 { 11 subnet_name = \u0026#34;subnet-01\u0026#34; 12 subnet_ip = \u0026#34;10.10.10.0/24\u0026#34; 13 subnet_region = \u0026#34;us-central1\u0026#34; 14 }, 15 { 16 subnet_name = \u0026#34;subnet-02\u0026#34; 17 subnet_ip = \u0026#34;10.10.20.0/24\u0026#34; 18 subnet_region = \u0026#34;us-central1\u0026#34; 19 subnet_private_access = \u0026#34;true\u0026#34; 20 subnet_flow_logs = \u0026#34;true\u0026#34; 21 description = \u0026#34;This subnet has a description\u0026#34; 22 } 23 ] 24} Run the following commands to initialize the module and create the VPC. Type yes at the prompt.\n1terraform init 2terraform apply Navigate to modules/instances/instances.tf. Replace the entire contents of the file with the following:\n1resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-1\u0026#34; { 2 name = \u0026#34;tf-instance-1\u0026#34; 3 machine_type = \u0026#34;n1-standard-2\u0026#34; 4 zone = var.zone 5 allow_stopping_for_update = true 6 7 boot_disk { 8 initialize_params { 9 image = \u0026#34;debian-cloud/debian-10\u0026#34; 10 } 11 } 12 13 network_interface { 14 network = \u0026#34;terraform-vpc\u0026#34; 15 subnetwork = \u0026#34;subnet-01\u0026#34; 16 } 17} 1819resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;tf-instance-2\u0026#34; { 20 name = \u0026#34;tf-instance-2\u0026#34; 21 machine_type = \u0026#34;n1-standard-2\u0026#34; 22 zone = var.zone 23 allow_stopping_for_update = true 24 25 boot_disk { 26 initialize_params { 27 image = \u0026#34;debian-cloud/debian-10\u0026#34; 28 } 29 } 30 31 network_interface { 32 network = \u0026#34;terraform-vpc\u0026#34; 33 subnetwork = \u0026#34;subnet-02\u0026#34; 34 } 35} Run the following commands to initialize the module and update the instances. Type yes at the prompt.\n1terraform init 2terraform apply Task - 6 : Configure a firewall Add the following resource to the main.tf file and fill in the GCP Project ID:\n1resource \u0026#34;google_compute_firewall\u0026#34; \u0026#34;tf-firewall\u0026#34; { 2 name = \u0026#34;tf-firewall\u0026#34; 3 network = \u0026#34;projects/\u0026lt;PROJECT_ID\u0026gt;/global/networks/terraform-vpc\u0026#34; 4 5 allow { 6 protocol = \u0026#34;tcp\u0026#34; 7 ports = [\u0026#34;80\u0026#34;] 8 } 9 10 source_tags = [\u0026#34;web\u0026#34;] 11 source_ranges = [\u0026#34;0.0.0.0/0\u0026#34;] 12} Run the following commands to configure the firewall. Type yes at the prompt.\n1terraform init 2terraform apply Congratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp345-automating-infrastructure-on-google-cloud-with-terraform/","title":"GSP-345 : Automating Infrastructure on Google Cloud with Terraform: Challenge Lab"},{"content":"Please do the following steps to complete this lab:\n1export PROJECT=$(gcloud info --format=\u0026#39;value(config.project)\u0026#39;) 2git clone https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis.git  Task - 1 : Create a Regional Cloud Storage bucket  1gsutil mb -p ${PROJECT} -l us-central1 gs://${PROJECT}-a  Task - 2 : Create a Cloud Function  1gcloud functions deploy safLongRunJobFunc \\ 2 --region us-central1 \\ 3 --trigger-resource gs://${PROJECT}-a \\ 4 --trigger-event google.storage.object.finalize \\ 5 --stage-bucket gs://${PROJECT}-a \\ 6 --source dataflow-contact-center-speech-analysis/saf-longrun-job-func \\ 7 --runtime nodejs10  Task - 3 : Create a BigQuery Dataset  1bq mk helpdesk  Task - 4 : Create Cloud Pub/Sub Topic  1gcloud pubsub topics create helpdesk  Task - 5 : Create a Cloud Storage Bucket for Staging Contents  1gsutil mb -p ${PROJECT} -l us-central1 gs://${PROJECT}-t 2mkdir DFaudio 3touch DFaudio/test 4gsutil cp -r DFaudio gs://${PROJECT}-t  Task - 6 : Deploy a Cloud Dataflow Pipeline  1python -m virtualenv env -p python3 2source env/bin/activate 3pip install apache-beam[gcp] 4pip install dateparser 5pip install Cython 6 7cd dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow/ 8export PROJECT=$(gcloud info --format=\u0026#39;value(config.project)\u0026#39;) 9python3 saflongrunjobdataflow.py \\ 10 --project ${PROJECT} \\ 11 --runner DataflowRunner \\ 12 --region us-central1 \\ 13 --temp_location gs://${PROJECT}-t/tmp \\ 14 --input_topic projects/${PROJECT}/topics/helpdesk \\ 15 --output_bigquery ${PROJECT}:helpdesk.realtime \\ 16 --requirements_file \u0026#34;requirements.txt\u0026#34; 17 18gcloud dataflow jobs list --region=us-central1  Task - 7 : Upload Sample Audio Files for Processing :-  1# mono flac audio sample 2gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:false -h x-goog-meta-pubsubtopicname:helpdesk -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_mono.flac gs://${PROJECT}-a 3 4# stereo wav audio sample 5gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:true -h x-goog-meta-pubsubtopicname:helpdesk -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_stereo.wav gs://${PROJECT}-a  Task - 8 : Run a Data Loss Prevention Job  1select * from (SELECT entities.name,entities.type, COUNT(entities.name) AS count FROM saf.transcripts, UNNEST(entities) entities GROUP BY entities.name, entities.type ORDER BY count ASC ) Where count \u0026gt; 5 Click “Save Query Results” and select “BigQuery table” option. Enter a name for a new table and save. Go to the new table in which result is saved and then Click on Export \u0026gt; Scan with DLP\nCongratulations, you're all done with the lab 😄\n","href":"https://www.courseintern.com/post/qwiklabs/challenge-labs/gsp311-automate-interactions-with-contact-center-ai/","title":"GSP-311 : Automate Interactions with Contact Center AI: Challenge Lab"},{"content":"Google Cloud Ready Facilitator Program Revealed! Hey Folks!\nThe Google Cloud Ready Facilitator Program is the program that provides all the opportunities to start up your career in the Google Cloud Platform. In this program, Google officially provides you all hands-on practice on Qwiklabs for the Google Cloud Platform. While doing this program you can grab and practice many concepts of computing, application development, big data, and machine learning. This program offers students a job-ready curriculum. This program will allow students to earn industry-recognized credentials. This program will reward the participants with free Google goodies, Google badges, Google Cloud certification, and free access to Google programs based on milestones achieved by the participants by completing quests and skill badges. If you get stuck while doing some quest and skill badges then our facilitators will always be available to help you out. Students who want to enhance their knowledge and skills in the field of Google Cloud Platform can unite in this program. This program helps the students to train them in industry-ready cloud skills and enhance their skills while participating in such programs. Students who want to become experts in Google Cloud and want to enhance their skills can participate in this program. Manifest your cloud skills by earning exclusive badges for your resume. To solve real-world problems Skill badges are shareable credentials that recognize your ability to solve real-world problems with your cloud knowledge.\nEligibility criteria to enroll in the program:  Working internet. Laptop with latest chrome browser. Age criteria should be 18+ Enroll in higher educational program in India.  To enroll in this program, the main thing to remember is that this program has a timeline from 10 April 2021 to 10 June 2021. You all can enroll in this program whether you are a fresher or an experienced person. The last date to fill the form is on 9th April till 11:59 PM. The form will require you to:\n Fill in personal and academic details with the size of the T-shirt. Submit the link to Qwiklabs Public profile. Go to the page and get the URL of your profile.  After filling the form you will get a confirmation on your g-mail. If you don't get the confirmation mail then wait for 15-30 min as well as check your spam box. If any participant needs to change any information in the form then they may reach out to the Facilitator and they may help them out.\nOnce you fill the form you will get a confirmation email with some instructions.\n  The instructions to avail of FREE credits pass on Qwiklabs. You need credits to take labs on Qwiklabs that are part of the program.\n  What's the last date of the program?\n  The timelines of the program.\n  What's the last date to get a chance at those rewards?\n  Other program-related information.\n  If you still didn't receive the confirmation mail then you must have provided the wrong Qwiklab profile URL above. If the link provided to you is not working in the enrolment email you can simply copy-paste the hardcoded URL in your browser beside each link in the mail and that should work too. If you don’t receive an invitation email after applying through the enrollment form then you may talk to the Facilitator. They may help you to enroll in the program.\nThis program has been divided into four milestones and individually milestones have their different prices. To earn the prizes in this program you have to complete the milestones.\n\u0026quot;THE HIGHER THE MILESTONES, THE HIGHER THE PRIZES\u0026quot; The milestones are  Milestone 1 -- To achieve milestone1 you need to complete any 8 quests and 4 skill badges in the program. As the reward for completing milestone1, you will be getting one google cloud t-shirt, Google cloud badge, Google cloud Pen, and Google cloud stickers. Milestone 2 -- To achieve milestone2 you need to complete any 16 quests and 8 skill badges in the program. As the reward for completing milestone2, you will be getting one sling bag, a Google cloud t-shirt, a Google Cloud badge, Google cloud Pen, and Google cloud stickers. Milestone3 -- To achieve milestone3 you need to complete any 24 quests and 12 skill badges in the program, As the reward for completing the milestone3 you will be getting one bag, sling bag, Google cloud t-shirt, Google cloud badge, Google cloud Pen, and Google cloud stickers. Milestone4 -- To achieve milestone4 you need to complete any 30 quests and 15 skill badges in the program, As the reward for completing the milestone4 you will be getting all the rewards of milestone1 to milestone3 but along with that you will be able to get free access to the Google Cloud Career Readiness program. You can get to learn more about this program” https://events.withgoogle.com/googlecloudready-facilitator-program/#content”.  Remember that you can receive the prizes only for the milestone achieved and not for the ones before that. If you complete any of the above-mentioned Milestone of the program prizes will be delivered to you within two months after the completion of the program to your own residential address.\nIf any participant has already received schwag in any of the Google Cloud Program like 30 days of Google Program 2020 will not be eligible to get prizes if they had completed any of the milestones. If you want to receive the prizes you need to complete the quest and skill badges after the enrollment period. Remember if you had completed it before it wouldn’t be counted. If you want you need to make a new account on Qwiklabs with a new email ID.\nYou might be thinking what is the difference between Quests and Skill badges. Quests have many training labs whereas skill badges it has training labs and a challenge lab too.\nFor better convenience, they have divided tracks into 3 parts.   Cloud Infrastructure Track - In Cloud Infrastructure Track you need to finish out any 10 quests and 8 skill badges in this track.\n  Cloud-native Application Development Track - In Cloud-native Application Development Track you need to utter any 10 quests and 3 skill badges in this track.\n  Big Data \u0026amp; Machine Learning Track - In Big Data \u0026amp; Machine Learning you need to finalize any 10 quests and 3 skill badges in this track.\n  ","href":"https://www.courseintern.com/post/google-courses/google-cloud-ready-facilitator-program/","title":"Google Cloud Ready Facilitator Program"}]